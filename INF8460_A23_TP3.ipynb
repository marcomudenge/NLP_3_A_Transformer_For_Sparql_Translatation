{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UG1IIiGzZSXy"
   },
   "source": [
    "## <center> École Polytechnique de Montréal <br> Département Génie Informatique et Génie Logiciel <br>  INF8460 – Traitement automatique de la langue naturelle <br> </center>\n",
    "## <center> TP3 - Interprétation de requêtes SPARQL par des questions en langue naturelle <br>  Automne 2023 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identification de l'équipe:\n",
    "\n",
    "### Groupe de laboratoire: \n",
    "\n",
    "### Equipe numéro : \n",
    "\n",
    "### Membres: \n",
    "\n",
    "- membre 1 (% de contribution, nature de la contribution)\n",
    "- membre 2 (% de contribution, nature de la contribution)\n",
    "- membre 3 (% de contribution, nature de la contribution)\n",
    "\n",
    "* nature de la contribution: Décrivez brièvement ce qui a été fait par chaque membre de l’équipe. Tous les membres sont censés contribuer au développement. Bien que chaque membre puisse effectuer différentes tâches, vous devez vous efforcer d’obtenir une répartition égale du travail. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJTHc5UnZSX0"
   },
   "source": [
    "## Description\n",
    "\n",
    "Dans ce laboratoire, vous allez construire un traducteur automatique en utilisant l'architecture du Transformeur. L'idée est d'utiliser un système de traduction automatique pour traduire des requêtes en langage SPARQL vers des questions en anglais.\n",
    "\n",
    "SPARQL est un langage d'interrogation de bases de connaissances, similaire à SQL. Les bases de connaissances sont une source de données structurées, selon les standards, modèles et langages du Web sémantique, qui permettent un accès efficace à une grande quantité d'information dans des domaines très variés. Cependant, leur accès est limité par la complexité des requêtes qui ne permet pas au public de s'en servir directement. Il est aussi difficile pour l'usager non averti de comprendre le sens d'une requête. Nous voulons donc coder un modèle de type Transformer qui permette d'interpréter une requête SPARQL sur la base de connaissances DBpedia en lui associant une question en anglais. \n",
    "\n",
    "Ainsi, notre système de traduction automatique prendra en entrée une requête SPARQL et produira en sortie une phrase en anglais correspondant à la question qui est posée par la requête. Par exemple :\n",
    "\n",
    "__Entrée__ _select distinct count ( ?uri ) where { dbr:Apocalypto dbo:language ?x . ?x dbp:region ?uri }_\n",
    "\n",
    "__Sortie attendue__ : _In how many other dbp:region do people live, whose dbo:language are spoken in dbr:Apocalypto?_\n",
    "\n",
    "Vous avez pu constater qu'on réutilise des éléments avec le préfixe dbr /dbo/dbp qui sont associés aux données dans DBpedia et au schéma de la base de connaissances. dbr:Apocalypto est tout simplement une URI qui décrit une ressource (ou donnée) dans DBpedia. Voici l'URI en question: https://dbpedia.org/describe/?url=http%3A%2F%2Fdbpedia.org%2Fresource%2FApocalypto&sid=35407\n",
    "\n",
    "Dans ce TP, vous reproduirez l'architecture du Transformer à l'aide de couches Keras. Vous pouvez vous inspirer de l'implémentation de certaines méthodes du tutoriel [Tensorflow](https://www.tensorflow.org/text/tutorials/transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ga8nmtY4ZSX1"
   },
   "source": [
    "## LIBRAIRIES PERMISES\n",
    "- Jupyter notebook\n",
    "- NLTK\n",
    "- Numpy\n",
    "- Pandas\n",
    "- Sklearn\n",
    "- Tensorflow\n",
    "- Keras\n",
    "- Transformers\n",
    "- Datasets\n",
    "- Pour toute autre librairie, demandez à votre chargé de laboratoire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi1tFmIZZSX1"
   },
   "source": [
    "## INFRASTRUCTURE\n",
    "\n",
    "- Vous avez accès aux GPU du local L-4818. Dans ce cas, vous devez utiliser le dossier temp (voir le tutoriel VirtualEnv.pdf)\n",
    "- Vous pouvez aussi utiliser Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHCvEjauZSX2"
   },
   "source": [
    "## DESCRIPTION DES DONNÉES ET MÉTRIQUES D’EVALUATION\n",
    "\n",
    "Le corpus est un corpus de 5 000 paires de questions - requêtes sur DBPedia portant sur une grande variété de thèmes plus ou moins spécifiques. Trois ensembles de données sont fournis :\n",
    "\n",
    "- Les 4000 paires de questions – requêtes d’entrainement dans un fichier `train.csv`.\n",
    "- Les 500 paires de questions – requêtes de validation dans un fichier `validation.csv`.\n",
    "- Les 500 paires de questions - requêtes de test dans un fichier `test.csv`\n",
    "\n",
    "La métrique BLEU sera utilisée pour comparer les traductions des modèles aux requêtes de référence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIZfzAD_ZSX2"
   },
   "source": [
    "## LABORATOIRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AHulKdPZZSX2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_text\n",
    "import pathlib\n",
    "import re\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-46mweTdajyw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root = os.getcwd() + \"\\\\\" # To change if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvMsQC9IZSX3"
   },
   "source": [
    "### 1 Préparation des données (10 points)\n",
    "\n",
    "Il faut tout d'abord préparer les données avant de les envoyer au système de traduction. Pour cela, deux classes seront utilisées. La classe `DataLoader` servira simplement à lire les données des fichiers d'entrainement et de validation et la classe `Preprocessor` servira à pré-traiter les données dans un format attendu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Yk674QnrZSX4"
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    Classe servant à charger les données en DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, training_path: str, validation_path: str) -> None:\n",
    "        # Initialise les attributs .train et .val en chargeant \n",
    "        # les données à partir des chemins d'accès donnés en paramètre\n",
    "        \n",
    "        self.train = pd.read_csv(training_path, sep=',', header=0).drop(columns=['id'])\n",
    "\n",
    "        self.val = pd.read_csv(validation_path, sep=',', header=0).drop(columns=['id'])\n",
    "\n",
    "    def get_train(self) -> pd.DataFrame:\n",
    "        # Retourne les données d'entraînement\n",
    "        return self.train\n",
    "    \n",
    "    def get_val(self) -> pd.DataFrame:\n",
    "        # Retourne les données de validation\n",
    "        return self.val\n",
    "    \n",
    "    def get_train_sparql(self) -> pd.Series:\n",
    "        # Retourne les données d'entraînement de la source\n",
    "        return self.train['sparql']\n",
    "    \n",
    "    def get_train_english(self) -> pd.Series:\n",
    "        # Retourne les données d'entraînement de la cible\n",
    "        return self.train['english']\n",
    "    \n",
    "    def get_val_sparql(self) -> pd.Series:\n",
    "        # Retourne les données de validation de la source\n",
    "        return self.val['sparql']\n",
    "    \n",
    "    def get_val_english(self) -> pd.Series:\n",
    "        # Retourne les données de validation de la cible\n",
    "        return self.val['english']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Pré-traitement\n",
    "\n",
    "La classe `Preprocessor` effectuera les transformations suivantes sur les requêtes SPARQL :\n",
    "- Remplacer tous les mots clés (préfixes) de la forme `dbx:` par `dbx_` (par exemple, `dbr:` devient `dbr_` et `dbo:` devient `dbo_`). Les mots clés qui doivent être pris en compte sont les suivants : `dbr`, `dbo`, `dbp` et `rdf`\n",
    "- Remplacer tous les signes de ponctuation suivants par des mots : \n",
    "  - `?` deviendra `var_`\n",
    "  - `{` deviendra `brack_open`\n",
    "  - `}` deviendra `brack_close`\n",
    "  - `(` deviendra `parent_open`\n",
    "  - `)` deviendra `parent_close`\n",
    "  - `.` deviendra `sep_dot`\n",
    "\n",
    "En ce qui concerne les questions en anglais, La classe `Preprocessor` effectuera les transformations suivantes :\n",
    "- Enlever les `?` à la fin des phrases\n",
    "- Remplacer tous les mots clés de la forme `dbx:` par `dbx_` (par exemple, `dbr:` devient `dbr_` et `dbo:` devient `dbo_`). Les mots clés qui doivent être pris en compte sont les suivants : `dbr`, `dbo`, `dbp` et `rdf`\n",
    "- Enlèvera tous les espaces inutiles avant le début et après la fin de la question \n",
    "\n",
    "Cette classe s'occupe aussi d'annuler le pré-traitement une fois que le Transformer aura généré une séquence, ce qui inclut notamment d'annuler les transformations indiquées ci-dessus et d'enlever les jetons de début et de fin de phrases qui auront été ajoutés par le segmenteur un peu plus bas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "20IAnMV4ZSX4"
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \"\"\"\n",
    "    Transforme et nettoie les données pour améliorer les performances du modèle\n",
    "    \"\"\"\n",
    "\n",
    "    SPARQL_TRANSLATE_OBJECTS = {\n",
    "        \"dbr:\": \"dbr_\",\n",
    "        \"dbo:\": \"dbo_\",\n",
    "        \"dbp:\": \"dbp_\",\n",
    "        \"rdf:\": \"rdf_\"\n",
    "    }\n",
    "\n",
    "    SPARQL_TRANSLATE_SYMBOLS = {\n",
    "        \"?\": \"var_\",\n",
    "        \"{\": \"brack_open\",\n",
    "        \"}\": \"brack_close\",\n",
    "        \"(\": \"parent_open\",\n",
    "        \")\": \"parent_close\",\n",
    "        \".\": \"sep_dot\"\n",
    "    }\n",
    "\n",
    "    def transform_dataframe(self, data: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Transforme les données d'une DataFrame contenant les colonnes 'english' \n",
    "        et 'sparql'. Fait appel aux fonctions `transform_sparql` et \n",
    "        `transform_english` sur les bonnes colonnes\n",
    "\n",
    "        Args :\n",
    "            - data : Données à transformer\n",
    "\n",
    "        Returns :\n",
    "            Données transformées\n",
    "        \"\"\"\n",
    "\n",
    "        # Transforme les données de la colonne 'sparql'\n",
    "        data['sparql'] = data['sparql'].apply(self.transform_sparql)\n",
    "\n",
    "        # Transforme les données de la colonne 'english'\n",
    "        data['english'] = data['english'].apply(self.transform_english)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def transform_sparql(self, sparql: str):\n",
    "        \"\"\"\n",
    "        Transforme une requête sparql en remplacant les jetons \"dbx:\" par \"dbx_\"\n",
    "        et en remplacant les signes de ponctuation par leur équivalent en mots \n",
    "        tel qu'indiqué plus haut\n",
    "\n",
    "        Args :\n",
    "            sparql : Requête sparql\n",
    "\n",
    "        Returns :\n",
    "            Requête sparql transformée avec les modifications mentionnées plus haut\n",
    "        \"\"\"\n",
    "        \n",
    "        # Remplace les SPARQL_TRANSLATE_OBJECTS dans la requête sparql\n",
    "        for key, value in self.SPARQL_TRANSLATE_OBJECTS.items():\n",
    "            sparql = sparql.replace(key, value)\n",
    "        \n",
    "        # Remplace les SPARQL_TRANSLATE_SYMBOLS dans la requête sparql\n",
    "        for key, value in self.SPARQL_TRANSLATE_SYMBOLS.items():\n",
    "            sparql = sparql.replace(key, value)\n",
    "\n",
    "        return sparql\n",
    "\n",
    "    def transform_english(self, english: str):\n",
    "        \"\"\"\n",
    "        Transforme une requête sparql en remplacant les jetons \"dbx:\" \n",
    "        par \"dbx_\" et en enlevant les points d'interrogation ainsi que \n",
    "        les espaces non-nécessaires au début et à la fin de la phrase\n",
    "\n",
    "        Args :\n",
    "            - english : Phrase en anglais sur laquelle appliquer \n",
    "            les transformations\n",
    "        \n",
    "        Returns :\n",
    "            Phrase transformée avec les modifications mentionnées plus haut\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Remplace les SQARQL_TRANSLATE_OBJECTS dans la question en anglais\n",
    "        for key, value in self.SPARQL_TRANSLATE_OBJECTS.items():\n",
    "            english = english.replace(key, value)\n",
    "\n",
    "        # Enlève les points d'interrogation\n",
    "        english = english.replace('?', '')\n",
    "\n",
    "        # Enlève les espaces au début et à la fin de la phrase\n",
    "        english = english.strip()\n",
    "\n",
    "        return english\n",
    "\n",
    "    def transform_back_english(self, english):\n",
    "        \"\"\"\n",
    "        Effectue les transformations inverses de la phrase en anglais \n",
    "        (remplace les dbx_ en dbx:).\n",
    "        Attention, cette fonction doit aussi enlever les jetons de début \n",
    "        et de fin d'une phrase qui sont ajoutés lors de \n",
    "        la segmentation (tokenization)\n",
    "\n",
    "        Args :\n",
    "            - english : Phrase générée par un modèle contenant les jetons \n",
    "            de début et de fin\n",
    "        \n",
    "        Returns :\n",
    "            - Phrase en anglais dont les transformations ont été annulées\n",
    "        \"\"\"\n",
    "        english = bytes(tf.squeeze(english).numpy()).decode()\n",
    "        \n",
    "        # Rétablie les SPARQL_TRANSLATE_OBJECTS dans la question en anglais\n",
    "        for key, value in self.SPARQL_TRANSLATE_OBJECTS.items():\n",
    "            english = english.replace(value, key)\n",
    "\n",
    "        return english\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez vérifier votre implémentation de la classe `Preprocessor` à l'aide du test suivant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed sparql : \n",
      "select distinct count parent_open var_uri parent_close where brack_open var_uri dbo_director dbr_Stanley_Kubrick sep_dot brack_close\n",
      "select distinct var_uri where brack_open var_uri dbo_founder dbr_John_Forbes_parent_openBritish_Army_officerparent_close sep_dot var_uri rdf_type dbo_City brack_close\n",
      "\n",
      "Transformed english : \n",
      "how many movies are there whose dbo_director is dbr_Stanley_Kubrick\n",
      "what dbo_City's dbo_founder is dbr_John_Forbes_(British_Army_officer)\n"
     ]
    }
   ],
   "source": [
    "def test_preprocessor():\n",
    "\n",
    "    test_queries = [\n",
    "        'select distinct count ( ?uri ) where { ?uri dbo:director dbr:Stanley_Kubrick . }',\n",
    "        'select distinct ?uri where { ?uri dbo:founder dbr:John_Forbes_(British_Army_officer) . ?uri rdf:type dbo:City }'\n",
    "    ]\n",
    "\n",
    "    test_english = [\n",
    "        'how many movies are there whose dbo:director is dbr:Stanley_Kubrick ?',\n",
    "        'what dbo:City\\'s dbo:founder is dbr:John_Forbes_(British_Army_officer) ?'\n",
    "    ]\n",
    "    \n",
    "    preprocessor = Preprocessor()\n",
    "    print('Transformed sparql : ')\n",
    "    for query in test_queries:\n",
    "        print(preprocessor.transform_sparql(query))\n",
    "\n",
    "    print()\n",
    "    print('Transformed english : ')\n",
    "    for english in test_english:\n",
    "        print(preprocessor.transform_english(english))\n",
    "\n",
    "\n",
    "test_preprocessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sortie attendue :\n",
    "\n",
    "```\n",
    "Transformed sparql : \n",
    "select distinct count parent_open var_uri parent_close where brack_open var_uri dbo_director dbr_Stanley_Kubrick sep_dot brack_close\n",
    "\n",
    "select distinct var_uri where brack_open var_uri dbo_founder dbr_John_Forbes_parent_openBritish_Army_officerparent_close sep_dot var_uri rdf_type dbo_City brack_close\n",
    "\n",
    "Transformed english : \n",
    "how many movies are there whose dbo_director is dbr_Stanley_Kubrick\n",
    "\n",
    "what dbo_City's dbo_founder is dbr_John_Forbes_(British_Army_officer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez maintenant instancier une objet de la classe `Data Loader` pour charger les données d'entrainement et de validation à partir des fichiers `train.csv` et `validation.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3L0LoK9-ZSX4"
   },
   "outputs": [],
   "source": [
    "# Instancier une objet de la classe DataLoader pour charger les données\n",
    "data_loader = DataLoader(\n",
    "    training_path=root + 'train.csv',\n",
    "    validation_path=root +'validation.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquez le pré-traitement des données sur les données chargées précédemment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "d1eNYoOTZSX4"
   },
   "outputs": [],
   "source": [
    "# Appliquer le pre-processeur sur les données d'entrainement et de validation\n",
    "\n",
    "pre_processor = Preprocessor()\n",
    "processed_train = pre_processor.transform_dataframe(data_loader.train)\n",
    "processed_val = pre_processor.transform_dataframe(data_loader.val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab9aHTYkZSX5"
   },
   "source": [
    "### 2. Segmentation (tokenization) (15 points)\n",
    "\n",
    "Une fois les données importées et modifiées, il faut adapter les phrases dans un format que le modèle peut comprendre.\n",
    "\n",
    "Tout d'abord, il faudra segmenter les phrases en jetons. Pour cela, un dictionnaire de mots (vocabulaire) sera nécessaire. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0 LanguageTokenizer (10 points)\n",
    "\n",
    "La classe `LanguageTokenizer` s'occupera de créer ce vocabulaire et de transformer les phrases d'un langage spécifique en jetons. Dans notre cas, il y aura 2 instances de cette classe : une pour l'anglais et l'autre pour sparql. Cette classe possède plusieurs fonctions qui nous seront très utiles notamment `create_vocab` pour créer le vocabulaire du modèle, `tokenize` pour transformer les phrases en jetons et `detokenize` pour transformer les jetons en phrases. \n",
    "\n",
    "Nous allons avoir recours au segmenteur de Bert pour trouver les jetons et le vocabulaire. Les paramètres du segmenteur vous sont donnés. Ce segmenteur divise chaque mot en parties de mots. Par exemple \"characteristically\" sera segmenté en 'characteristic' et '##ally'. \n",
    "\n",
    "Ensuite, pour chacune des phrases, après les avoir transformées en jetons, il faudra ajouter les jetons de début (`[START]`) et de fin de phrase (`[END]`). Cette opération sera effectuée dans la fonction `add_start_end`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "aQsquspb7esn"
   },
   "outputs": [],
   "source": [
    "class LanguageTokenizer(tf.Module):\n",
    "    \"\"\"\n",
    "    Classe représentant un tokenizer pour un langage spécifique. \n",
    "    Dans notre cas, il y en aura un pour sparql et un pour l'anglais\n",
    "    \"\"\"\n",
    "\n",
    "    reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "    START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
    "    END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
    "\n",
    "    tokenizer_params = dict(lower_case=True)\n",
    "\n",
    "    vocab_args = dict(\n",
    "        vocab_size = 8000,\n",
    "        reserved_tokens=reserved_tokens,\n",
    "        bert_tokenizer_params=tokenizer_params,\n",
    "        learn_params=None,\n",
    "    )\n",
    "\n",
    "    def __init__(self, reserved_tokens, vocab_path):\n",
    "        \"\"\"\n",
    "        Initialise le BertTokenizer en utilisant le paramètre `vocab_path` \n",
    "        et en mettant le tokenizer en mode \"lower case\".\n",
    "\n",
    "        Args :\n",
    "            - reserved_tokens : Jetons réservés du BertTokenizer\n",
    "            - vocab_path : Chemin vers le fichier contenant le vocabulaire du tokenizer\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__(name=\"LanguageTokenizer\")\n",
    "\n",
    "        with open(vocab_path) as f:\n",
    "            f = open(vocab_path, 'r')\n",
    "\n",
    "            init = tf.lookup.TextFileInitializer(\n",
    "                    f.name,\n",
    "                    key_dtype=tf.string, key_index=tf.lookup.TextFileIndex.WHOLE_LINE,\n",
    "                    value_dtype=tf.int64, value_index=tf.lookup.TextFileIndex.LINE_NUMBER)\n",
    "\n",
    "            lookup_table = tf.lookup.StaticVocabularyTable(\n",
    "                init,\n",
    "                num_oov_buckets=1\n",
    "            )\n",
    "\n",
    "            f.close()\n",
    "        \n",
    "        # Initialize the BertTokenizer\n",
    "        self.tokenizer = tensorflow_text.BertTokenizer(lookup_table, **self.tokenizer_params)\n",
    "        \n",
    "        # Initialise les jetons réservés\n",
    "        self.reserved_tokens = reserved_tokens\n",
    "\n",
    "    def create_vocab(language_sentences: pd.DataFrame, path: str):\n",
    "        \"\"\"\n",
    "        Crée un vocabulaire à partir des phrases en entrée \n",
    "        (language_sentences). Pour cela vous devrez utiliser \n",
    "        la fonction bert_vocab_from_dataset(). Attention, il \n",
    "        ne faut pas oublier de passer en paramètres `vocab_args` \n",
    "        à la fonction qui s'occupe de créer le vocabulaire.\n",
    "\n",
    "        Une fois le vocabulaire créé, il faudra le sauvegarder dans un fichier spécifié\n",
    "        par l'attribut `path`.\n",
    "\n",
    "        Args :\n",
    "            - language_sentences : DataFrame contenant les phrases du langage\n",
    "            - path : Chemin où sera sauvegardé le vocabulaire\n",
    "        \"\"\"\n",
    "        \n",
    "        # On convertie les phrases en un dataset (Tensorflow)\n",
    "        vocab_tf_dataset = tf.data.Dataset.from_tensor_slices((language_sentences))\n",
    "\n",
    "        # Création du vocabulaire\n",
    "        vocab = bert_vocab.bert_vocab_from_dataset(vocab_tf_dataset, **LanguageTokenizer.vocab_args)\n",
    "        \n",
    "        # Save the vocabulary to the specified file\n",
    "        with open(path, 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(vocab))\n",
    "            f.close()\n",
    "\n",
    "    @tf.function\n",
    "    def tokenize(self, inputs):\n",
    "        \"\"\"\n",
    "        Transforme des phrases en index de jetons et qui ajoute les \n",
    "        jetons de début et de fin.\n",
    "        \n",
    "        Args :\n",
    "            - inputs : Phrases d'entrée\n",
    "\n",
    "        Returns :\n",
    "            Jetons correspondant à la phrase avec les jetons de début et de fin\n",
    "        \"\"\"\n",
    "\n",
    "        # Tokenize the inputs into a ragged tensor\n",
    "        tokens = self.tokenizer.tokenize(inputs).merge_dims(1,2) # TODO: CHECK IF THERE'S A WAY TO AVOID THE MERGE\n",
    "        \n",
    "        # Add the start and end token to the tokenized inputs\n",
    "        tokens = LanguageTokenizer.add_start_end(tokens)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    @tf.function\n",
    "    def detokenize(self, tokenized):\n",
    "        \"\"\"\n",
    "        Transforme une liste d'index en jetons. Applique ensuite \n",
    "        la méthode `cleanup_text` du pour nettoyer \n",
    "        les données.\n",
    "\n",
    "        Args :\n",
    "            - tokenized : Liste de jetons\n",
    "\n",
    "        Returns :\n",
    "            Phrase correspondant aux jetons\n",
    "        \"\"\"\n",
    "\n",
    "        # Detokenize: turn the token IDs back into text returned as a ragged tensor\n",
    "        detokenized = self.tokenizer.detokenize(tokenized)\n",
    "\n",
    "        # Cleaning the detokenized text tensor back into a regular string\n",
    "        detokenized = LanguageTokenizer.cleanup_text(self.reserved_tokens, detokenized)\n",
    "\n",
    "        return detokenized\n",
    "\n",
    "    def add_start_end(tokenized_sentences):\n",
    "        \"\"\"\n",
    "        Fonction qui ajoute la représentation des tokens [START] et [END] à la phrase en entrée\n",
    "        \n",
    "        Args :\n",
    "            - tokenized_sentences: Tenseur contenant les indices des jetons des phrases\n",
    "\n",
    "        Returns :\n",
    "            Tenseur initial avec les indices des jetons [START] et [END] au début et à la fin\n",
    "        \"\"\"\n",
    "\n",
    "        # Create the start and end tokens tensors. Dimensions should match the number of sentences\n",
    "        start = tf.fill([tokenized_sentences.bounding_shape()[0], 1], LanguageTokenizer.START)\n",
    "        end = tf.fill([tokenized_sentences.bounding_shape()[0], 1], LanguageTokenizer.END)\n",
    "\n",
    "        # Make the data type compatible with the tokenized_sentences tensor\n",
    "        start = tf.cast(start, tf.int64)\n",
    "        end = tf.cast(end, tf.int64)\n",
    "\n",
    "        # Concatenate the start tokens, sentences and end tokens\n",
    "        tokenized_sentences = tf.concat([start, tokenized_sentences, end], axis=1)\n",
    "\n",
    "        return tokenized_sentences\n",
    "\n",
    "\n",
    "    def cleanup_text(reserved_tokens, token_txt):\n",
    "        \"\"\"\n",
    "        Fonction qui nettoie un texte généré par la fonction detokenize() du BertTokenizer.\n",
    "        Args :\n",
    "            - reserved_tokens : Jetons réservés du BertTokenizer\n",
    "            - token_text : Chaine généré par la fonction detokenize()\n",
    "\n",
    "        Returns :\n",
    "            Texte nettoyé\n",
    "        \"\"\"\n",
    "        \n",
    "        # Determine which tokens are reserved based on the reserved_tokens\n",
    "        bad_tokens = [re.escape(token) for token in reserved_tokens if token != \"[UNK]\"] # Tokens to remove are all non-[UNK] tokens\n",
    "\n",
    "        # Create a regular expression pattern from the bad_tokens\n",
    "        bad_token_regex = tf.strings.join(bad_tokens, \"|\")\n",
    "\n",
    "        # Determine which tokens match the bad_token_regex\n",
    "        bad_cells = tf.strings.regex_full_match(token_txt, bad_token_regex)\n",
    "\n",
    "        # Replace the bad tokens with an empty string\n",
    "        cleaned_cells = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
    "\n",
    "        # Joining the good cells back into text\n",
    "        cleaned_cells = tf.strings.reduce_join(cleaned_cells, separator=' ', axis=-1)\n",
    "\n",
    "        return cleaned_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 320, 24, ..., 23, 21, 3], [2, 43, 45, 102, 30, 3]]\n"
     ]
    }
   ],
   "source": [
    "def test_add_start_end():\n",
    "\n",
    "    tokenized_sentence = tf.ragged.constant([[320, 24, 500, 23, 21], [43, 45, 102, 30]], dtype=tf.int64)\n",
    "    tf.print(LanguageTokenizer.add_start_end(tokenized_sentence))\n",
    "\n",
    "test_add_start_end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sortie attendue : \n",
    "```\n",
    "[[2, 320, 24, ..., 23, 21, 3], [2, 43, 45, 102, 30, 3]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary :  [PAD] [UNK] [START] [END] . ? a b d e h i k m n o p r s t u w y ##. ##? ##a ##b ##d ##e ##h ##i ##k ##m ##n ##o ##p ##r ##s ##t ##u ##w ##y\n",
      "Tokenized sentence : <tf.RaggedTensor [[2, 10, 34, 40, 13, 25, 33, 41, 20, 4, 18, 16, 36, 28, 37, 30, 27, 28,\n",
      "  33, 38, 37, 21, 28, 36, 28, 7, 34, 36, 33, 11, 33, 14, 28, 40, 22, 34,\n",
      "  36, 31, 5, 3]]>\n",
      "Detokenized sentence : how many u . s presidents were born in new york ?\n"
     ]
    }
   ],
   "source": [
    "def test_tokenizer():\n",
    "\n",
    "    sentence = ['how many U.S Presidents were born in New York ?']\n",
    "    vocab_path = root + 'test_language_vocab.txt'\n",
    "    LanguageTokenizer.create_vocab(sentence, vocab_path)\n",
    "\n",
    "    with open(vocab_path) as f:\n",
    "        vocab = f.read()\n",
    "\n",
    "    print('Vocabulary : ', vocab.replace('\\n', ' '))\n",
    "    test_tokenizer_obj = LanguageTokenizer(LanguageTokenizer.reserved_tokens, vocab_path)\n",
    "    tokenized_sentence = test_tokenizer_obj.tokenize(sentence)\n",
    "    tf.print(f'Tokenized sentence : {tokenized_sentence}')\n",
    "\n",
    "    detokenized_sentence = test_tokenizer_obj.detokenize(tokenized_sentence)\n",
    "    tf.print(f'Detokenized sentence : {bytes(tf.squeeze(detokenized_sentence).numpy()).decode()}') # TODO: Would fail for batched inputs\n",
    "\n",
    "test_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sortie attendue : \n",
    "```\n",
    "Vocabulary :  [PAD] [UNK] [START] [END] . ? a b d e h i k m n o p r s t u w y ##. ##? ##a ##b ##d ##e ##h ##i ##k ##m ##n ##o ##p ##r ##s ##t ##u ##w ##y \n",
    "Tokenized sentence : <tf.RaggedTensor [[2, 10, 34, 40, 13, 25, 33, 41, 20, 4, 18, 16, 36, 28, 37, 30, 27, 28,\n",
    "  33, 38, 37, 21, 28, 36, 28, 7, 34, 36, 33, 11, 33, 14, 28, 40, 22, 34,\n",
    "  36, 31, 5, 3]]>\n",
    "Detokenized sentence : how many u . s presidents were born in new york ?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Vocabulaire  (5 points) \n",
    "\n",
    "Vous pouvez maintenant créer le vocabulaire de chaque langage à l'aide de la fonction `create_vocab`. Vous pouvez stocker le vocabulaire anglais dans un fichier appelé `language_vocab_english.txt` et le vocabulaire sparql dans un fichier appelé `language_vocab_sparql.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "55klqAgxCH_Q"
   },
   "outputs": [],
   "source": [
    "sparql_vocab_path = root + 'language_vocab_sparql.txt'\n",
    "english_vocab_path = root + 'language_vocab_english.txt'\n",
    "\n",
    "# Create the vocabulary for the sparql and english sentences\n",
    "LanguageTokenizer.create_vocab(processed_train['sparql'], sparql_vocab_path)\n",
    "LanguageTokenizer.create_vocab(processed_train['english'], english_vocab_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de n'utiliser qu'une seule classe, nous allons créer une classe qui regroupe les deux tokenizers en une seule classe appelée `GroupedTokenizers`. Complétez le constructeur qui initialise l'attribut `english` correspondant au tokenizer anglais et l'attribut `sparql` correspondant au tokenizer sparql."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "eza5AHd_C054"
   },
   "outputs": [],
   "source": [
    "class GroupedTokenizers(tf.Module):\n",
    "    \"\"\"\n",
    "    Cette classe regroupe les deux segmenteurs (tokenizers) qui seront \n",
    "    utilisés (une pour chacun des langages)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reserved_tokens, vocab_english_path: str, vocab_sparql_path: str):\n",
    "        \"\"\"\n",
    "        Initialise les deux tokenizers (english and sparql)\n",
    "        Args :\n",
    "            - reserved_tokens : Jetons réservés du BertTokenizer\n",
    "            - vocab_english_path : Chemin vers le fichier contenant \n",
    "            le vocabulaire anglais du segmenteur (tokenizer)\n",
    "            - vocab_sparql_path : Chemin vers le fichier contenant \n",
    "            le vocabulaire sparql du segmenteur (tokenizer)\n",
    "        \"\"\"\n",
    "        self.english = LanguageTokenizer(reserved_tokens, vocab_english_path)\n",
    "        self.sparql = LanguageTokenizer(reserved_tokens, vocab_sparql_path)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le test suivant permet de vérifier que votre pré-traitement et votre tokenizer fonctionnent correctement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BVLoJCRLDhVX",
    "outputId": "9c3cb250-b3f3-4588-b0ac-78da0f042fbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English : \n",
      " how many movies are there whose dbo:director is dbr:Stanley_Kubrick ? \n",
      "\n",
      "Processed english : \n",
      " how many movies are there whose dbo_director is dbr_Stanley_Kubrick \n",
      "\n",
      "Tokenized english : \n",
      " <tf.RaggedTensor [[2, 74, 75, 495, 67, 73, 65, 61, 25, 228, 59, 60, 25, 896, 95, 261, 25,\n",
      "  36, 116, 329, 757, 114, 3]]> \n",
      "\n",
      "Detokenized english : \n",
      " 0    how many movies are there whose dbo _ director...\n",
      "dtype: object \n",
      "\n",
      "\n",
      "------------------------------------------------\n",
      "\n",
      "Sparql : \n",
      " select distinct count ( ?uri ) where { ?uri dbo:director dbr:Stanley_Kubrick . } \n",
      "\n",
      "Processed sparql : \n",
      " select distinct count parent_open var_uri parent_close where brack_open var_uri dbo_director dbr_Stanley_Kubrick sep_dot brack_close \n",
      "\n",
      "Tokenized sparql : \n",
      " <tf.RaggedTensor [[2, 66, 65, 71, 68, 22, 63, 55, 22, 56, 68, 22, 62, 64, 57, 22, 63, 55,\n",
      "  22, 56, 61, 22, 208, 59, 22, 41, 182, 80, 233, 22, 33, 879, 703, 111,\n",
      "  60, 22, 58, 57, 22, 62, 3]]> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizers = GroupedTokenizers(\n",
    "    LanguageTokenizer.reserved_tokens,\n",
    "    root + 'language_vocab_english.txt',\n",
    "    root + 'language_vocab_sparql.txt'\n",
    ")\n",
    "\n",
    "def test_tokenizer_preprocessor(tokenizers: GroupedTokenizers):\n",
    "    \"\"\"\n",
    "    Verifie que les fonctions du tokenizer et du preprocessor sont belles \n",
    "    et bien codées. Si elles le sont, les phrases initiales anglaises et \n",
    "    sparql devraient être identiques à celles en entrée\n",
    "\n",
    "    \"\"\"\n",
    "    english = 'how many movies are there whose dbo:director is dbr:Stanley_Kubrick ?'\n",
    "    sparql = 'select distinct count ( ?uri ) where { ?uri dbo:director dbr:Stanley_Kubrick . }'\n",
    "    print('English : \\n', english, '\\n')\n",
    "\n",
    "    # processed_train = pre_processor.transform_dataframe\n",
    "    pre_processor = Preprocessor()\n",
    "    processed_english = pre_processor.transform_english(english)\n",
    "    processed_sparql = pre_processor.transform_sparql(sparql)\n",
    "\n",
    "    print('Processed english : \\n', processed_english, '\\n')\n",
    "    tokenized_english = tokenizers.english.tokenize(processed_english)\n",
    "    print('Tokenized english : \\n', tokenized_english, '\\n')\n",
    "    detokenized_english = pd.Series(tokenizers.english.detokenize(tokenized_english).numpy())\n",
    "    print('Detokenized english : \\n', detokenized_english.apply(pre_processor.transform_back_english), '\\n')\n",
    "    print()\n",
    "    print('------------------------------------------------')\n",
    "    print()\n",
    "\n",
    "    print('Sparql : \\n', sparql, '\\n')\n",
    "\n",
    "    print('Processed sparql : \\n', processed_sparql, '\\n')\n",
    "    tokenized_sparql = tokenizers.sparql.tokenize(processed_sparql)\n",
    "    print('Tokenized sparql : \\n', tokenized_sparql, '\\n')\n",
    "\n",
    "test_tokenizer_preprocessor(tokenizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njcViPKN1ndG"
   },
   "source": [
    "### 3. Création de lots (Batching) (5 points)\n",
    "\n",
    "Étant donnée la grande quantité de données impliquant l'entrainement d'un modèle, il est important d'envoyer les données de la manière la plus efficace possible. Pour cela, les données sont regroupées en petits groupes appelés \"batchs\" ou lots. Cela permet notamment de traiter plusieurs éléments en parallèle et réduit considérablement le temps d'entrainement.\n",
    "\n",
    "Pour cela, la classe `Batcher` sera utilisée. Cette classe s'occupe de regrouper les données en petits lots et de les préparer pour les envoyer au modèle. Cette classe possède plusieurs fonctions :\n",
    "- `make_batches`: Elle reçoit en paramètre une instance de la classe `tf.Dataset`. Elle divise ensuite le dataset en petits lots et les envoie à la fonction `prepare_batch` \n",
    "- `prepare_batch`: Reçoit un lot/\"batch\" et le prépare en effectuant les transformations suivantes :\n",
    "  - Segmente les phrases en entrée en utilisant les bons tokenizers passés en paramètres dans le constructeur\n",
    "  - S'assure que la taille des phrases ne dépasse pas `max_tokens`\n",
    "\n",
    "\n",
    "<img src=\"Batcher.png\" alt=\"Batcher\" width=\"100%\" height=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "y0jovaYiEdh8"
   },
   "outputs": [],
   "source": [
    "class Batcher():\n",
    "    \"\"\"\n",
    "    Cette classe s'occupe de regrouper les données en petits groupes (batches) et \n",
    "    de préparer les données pour les envoyer au modèle.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizers: GroupedTokenizers, train, max_tokens, batch_size, buffer_size):\n",
    "        \"\"\"\n",
    "        Initialise les paramètres en entrée \n",
    "\n",
    "        Args :\n",
    "            - tokenizers : tokenizers pour transformer les entrées en jeton\n",
    "            - train : Valeur booléenne pour savoir si les batches seront utilisées \n",
    "            pour de l'entrainement ou pas\n",
    "            - max_tokens : Nombre de jetons maximums pour une entrée\n",
    "            - batch_size : Taille des groupes (batches)\n",
    "            - buffer_size : Taille du buffer servant à mélanger les données dans le \n",
    "            cas de l'entrainement\n",
    "        \"\"\"\n",
    "\n",
    "        self.tokenizers = tokenizers\n",
    "        self.train = train\n",
    "        self.max_tokens = max_tokens\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def prepare_batch(self, input_language, output_language=None):\n",
    "        \"\"\"\n",
    "        Prépare les batches pour les envoyer au modèle. Cette fonction est \n",
    "        appelée pour chaque élément d'un Tensorflow Dataset.\n",
    "\n",
    "        Effectue les transformations suivantes :\n",
    "            - Tokenize les phrases en entrées en utilisant les bons tokenizers passés \n",
    "            en paramètre dans le constructeur\n",
    "            - S'assure que la taille des phrases ne dépasse pas `max_tokens` (max_tokens \n",
    "            est inclus)\n",
    "\n",
    "        Args :\n",
    "            - input_language : Entrée dans le langage d'entrée (sparql dans notre cas) \n",
    "            de la taille (self.batch_size, x)\n",
    "            - output_language : Sortie dans le langage de sortie (english dans notre cas) \n",
    "            de la taille (self.batch_size, x). None dans le cas de batches de test\n",
    "\n",
    "        Returns :\n",
    "            - Si self.train == True :\n",
    "                Retourne un tuple de la forme ((input_langage, output_language_inputs), output_language_labels)\n",
    "                qui seront les entrées respectives de l'encodeur et du décodeur et la \n",
    "                sortie du décodeur.\n",
    "\n",
    "                Voici ce que chaque valeur de retour représente\n",
    "                - input_language : tenseur contenant les jetons du paramètre `input_language` \n",
    "                limité à `max_tokens`\n",
    "                - output_language_inputs : tenseur contenant les jetons du paramètre \n",
    "                `output_language` limité à `max_tokens`+1 (pour permettre de prédire le prochain \n",
    "                jeton)\n",
    "                - output_language_labels : tenseur contenant les jetons du paramètre \n",
    "                `output_language` contenant le prochain charactère\n",
    "                \n",
    "            - Si self.train == False :\n",
    "                Retourne un tuple de la forme (input_language, output_language) qui \n",
    "                représentent l'entrée de l'encodeur et un\n",
    "                tenseur de sortie initialisé avec le jeton d'entrée de la taille \n",
    "                (self.batch_size,). Les valeurs de retour sont expliquées plus haut\n",
    "        \"\"\"\n",
    "        # TODO: wait for moodle answer before fixing bug\n",
    "        input_language = self.tokenizers.sparql.tokenize(input_language)\n",
    "        input_language = input_language.to_tensor()\n",
    "        input_language = input_language[:, :self.max_tokens]\n",
    "\n",
    "        if (self.train == True):\n",
    "            output_language_inputs = self.tokenizers.english.tokenize(output_language)\n",
    "            output_language_inputs = output_language_inputs.to_tensor()\n",
    "            output_language_labels = output_language_inputs[:, :self.max_tokens+1]\n",
    "            output_language_inputs = output_language_inputs[:, :self.max_tokens]\n",
    "            return ((input_language, output_language_inputs), output_language_labels)\n",
    "        else:\n",
    "            output_language = tf.fill([input_language.shape[0], 1], LanguageTokenizer.START)\n",
    "            return (input_language, output_language)\n",
    "    \n",
    "    def make_batches(self, ds):\n",
    "        \"\"\"\n",
    "        Args :\n",
    "            - ds : Dataset contenant les examples de la forme \n",
    "            ((sparql, english_in), english_label)\n",
    "            si self.train == True et de la forme (sparql, english) \n",
    "            si self.train == False\n",
    "\n",
    "        Returns :\n",
    "            Le dataset initial (mélangé si self.train == True) contenant des \n",
    "            éléments de la taille de self.batch_size dont la fonction self.prepare_batch \n",
    "            a été appelée sur chacun des éléments et dont les éléments sont \n",
    "            pré-récupéré (prefetched). Si self.train == False, c'est le même principe,\n",
    "            mais les données ne sont pas mélangées\n",
    "        \"\"\"\n",
    "        # TODO: wait for moodle answer before fixing bug\n",
    "        if (self.train == True):\n",
    "            ds = ds.shuffle(self.buffer_size)\n",
    "        ds = ds.map(self.prepare_batch)\n",
    "        ds = ds.batch(self.batch_size)\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez maintenant tester le batcher à l'aide de la fonction suivante (vérifiez bien que la sortie du décodeur contient un jeton de plus que la phrase qui entre dans le décodeur et que ce qui rentre dans l'encodeur est bel et bien du sparql)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yoo\n",
      "(2, 1, 8)\n",
      "(2, 1, 8)\n",
      "(2, 1, 9)\n",
      "Detokenized inputs encoder :  [['select distinct count parent _ open var'], ['select distinct var _ uri where brack']]\n",
      "Detokenized inputs decoder  :  [['how many movies are there whose dbo'], ['what is the dbo _ river whose']]\n",
      "Detokenized outputs decoder :  [['how many movies are there whose dbo _'], ['what is the dbo _ river whose dbo']]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__ConcatV2_N_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Dimension 2 in both shapes must be equal: shape[0] = [2,1,8] vs. shape[2] = [2,1,9] [Op:ConcatV2] name: concat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32me:\\TP A2023\\INF8460\\INF8460_TP3\\INF8460_A23_TP3.ipynb Cell 37\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TP%20A2023/INF8460/INF8460_TP3/INF8460_A23_TP3.ipynb#X51sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         concat \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconcat([x[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m], x[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m], x[\u001b[39m1\u001b[39m]], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TP%20A2023/INF8460/INF8460_TP3/INF8460_A23_TP3.ipynb#X51sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mConcatened values : \u001b[39m\u001b[39m'\u001b[39m, concat)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/TP%20A2023/INF8460/INF8460_TP3/INF8460_A23_TP3.ipynb#X51sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m test_batcher(tokenizers)\n",
      "\u001b[1;32me:\\TP A2023\\INF8460\\INF8460_TP3\\INF8460_A23_TP3.ipynb Cell 37\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TP%20A2023/INF8460/INF8460_TP3/INF8460_A23_TP3.ipynb#X51sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m tf\u001b[39m.\u001b[39mprint(\u001b[39m'\u001b[39m\u001b[39mDetokenized inputs decoder  : \u001b[39m\u001b[39m'\u001b[39m, tokenizers\u001b[39m.\u001b[39menglish\u001b[39m.\u001b[39mdetokenize(x[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TP%20A2023/INF8460/INF8460_TP3/INF8460_A23_TP3.ipynb#X51sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m tf\u001b[39m.\u001b[39mprint(\u001b[39m'\u001b[39m\u001b[39mDetokenized outputs decoder : \u001b[39m\u001b[39m'\u001b[39m, tokenizers\u001b[39m.\u001b[39menglish\u001b[39m.\u001b[39mdetokenize(x[\u001b[39m1\u001b[39m]))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/TP%20A2023/INF8460/INF8460_TP3/INF8460_A23_TP3.ipynb#X51sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m concat \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mconcat([x[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m], x[\u001b[39m0\u001b[39;49m][\u001b[39m1\u001b[39;49m], x[\u001b[39m1\u001b[39;49m]], axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TP%20A2023/INF8460/INF8460_TP3/INF8460_A23_TP3.ipynb#X51sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mConcatened values : \u001b[39m\u001b[39m'\u001b[39m, concat)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\ops.py:7209\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7207\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7208\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 7209\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__ConcatV2_N_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Dimension 2 in both shapes must be equal: shape[0] = [2,1,8] vs. shape[2] = [2,1,9] [Op:ConcatV2] name: concat"
     ]
    }
   ],
   "source": [
    "def test_batcher(tokenizers):\n",
    "\n",
    "\n",
    "    english = pd.Series([\n",
    "        'how many movies are there whose dbo_director is dbr_Stanley_Kubrick', \n",
    "        'what is the dbo_River whose dbo_riverMouth is dbr_Dead_Sea', \n",
    "    ])\n",
    "\n",
    "    sparql = pd.Series([\n",
    "        'select distinct count parent_open var_uri parent_close where brack_open var_uri dbo_director dbr_Stanley_Kubrick sep_dot brack_close', \n",
    "        'select distinct var_uri where brack_open var_uri dbo_riverMouth dbr_Dead_Sea sep_dot var_uri rdf_type dbo_River brack_close', \n",
    "    ])\n",
    "    \n",
    "    batcher = Batcher(tokenizers, True, 8, 64, 20000)\n",
    "\n",
    "    val_english = tf.data.Dataset.from_tensor_slices(english)\n",
    "    val_sparql = tf.data.Dataset.from_tensor_slices(sparql)\n",
    "    val_examples = tf.data.Dataset.zip((val_sparql, val_english))\n",
    "\n",
    "    batches = batcher.make_batches(val_examples)\n",
    "    for x in batches:\n",
    "        print('yoo')\n",
    "        print(x[0][0].shape)\n",
    "        print(x[0][1].shape)\n",
    "        print(x[1].shape)\n",
    "        tf.print('Detokenized inputs encoder : ', tokenizers.sparql.detokenize(x[0][0]))\n",
    "        tf.print('Detokenized inputs decoder  : ', tokenizers.english.detokenize(x[0][1]))\n",
    "        tf.print('Detokenized outputs decoder : ', tokenizers.english.detokenize(x[1]))\n",
    "\n",
    "        concat = tf.concat([x[0][0], x[0][1], x[1]], axis=1)\n",
    "        print('Concatened values : ', concat)\n",
    "\n",
    "test_batcher(tokenizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Uy4egm3N_py"
   },
   "source": [
    "### 4. Transformer (30 points)\n",
    "\n",
    "<img style=\"float: right;\" src=\"Transformer.png\" alt=\"Transformer\" width=\"500\" height=\"700\"/>\n",
    "\n",
    "\n",
    "Maintenant que les données sont prêtes à être envoyées au modèle, il ne manque qu'à créer son architecture. Pour cela, la librairie Keras sera utilisée. Keras est une librairie qui est construite au dessus de Tensorflow pour faciliter le développement de modèles dans un style orienté objet. Depuis Tensorflow 2.0, elle est maintenant directement intégrée à Tensorflow. Pour plus de détails, la documentation est présente sur ce [site](https://keras.io/api/)\n",
    "\n",
    "L'architecture qui sera suivie dans ce TP est présentée dans l'image à droite. La liste des couches qui seront implémentées sont les suivantes : \n",
    "- `Positional Embedding` : Permet la génération des plongements de position\n",
    "- `Global-Self Attention` : S'occupe du mécanisme d'attention de l'encodeur\n",
    "- `Feed Forward` : Permet de connecter des entrées et des sorties avec un réseau de neurones\n",
    "- `Decoder Attention` : S'occupe du premier mécanisme d'attention du décodeur\n",
    "- `Cross Attention` : S'occupe du deuxième mécanisme d'attention du décodeur (relie l'encodeur au décodeur)\n",
    "\n",
    "Les couches d'addition et de normalisation seront incluses dans les couches précédentes. Par exemple, la couche `Add & Norm` qui suit la couche `Global-Self Attention` dans le graphique sera inclus dans la couche `Global-Self Attention`.\n",
    "\n",
    "Ensuite, des couches seront égalements utilisées pour regrouper ces couches pour simplifier le pipeline du Transformer. Voici la liste des couches qui seront ajoutées à celles sur le graphique :\n",
    "- `Encoder Layer` : Représente un seul encodeur contenant les couches `Global-Self Attention` et `Feed Forward`\n",
    "- `Decoder Layer` : Représente un seul décodeur contenant les couches `Decoder Attention`, `Cross Attention` et `Feed Forward` \n",
    "- `Encoder` : Représente plusieurs encodeurs en parallèle \n",
    "- `Decoder` : Représente plusieurs décodeurs en parallèle\n",
    "- `Transformer` : Représente le Transformer au complet et regroupe tous les encodeurs, décodeurs et les couches de plongements\n",
    "\n",
    "Chaque couche sera créée manuellement et implémentée en tant que couche Keras. Si vous n'êtes pas familier avec Keras, voici quelques tutoriels qui pourraient vous aider :\n",
    "- https://keras.io/api/models/model/\n",
    "- https://www.tensorflow.org/text/tutorials/transformer\n",
    "- https://machinelearningmastery.com/implementing-the-transformer-encoder-from-scratch-in-tensorflow-and-keras/\n",
    "\n",
    "\n",
    "Les classes sont déjà créées pour vous et vous n'aurez principalement qu'à implémenter la fonction `call()` de chacune de ces classes. La fonction `call()` s'occupe, pour une couche donnée, de transformer une entrée en sortie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Z4VEorxV_DL"
   },
   "source": [
    "#### 4.1 Positional Embedding  \n",
    "\n",
    "Pour permettre au modèle de prendre en compte l'ordre des jetons qui lui sont passés, il est important de passer de l'information au modèle à propos de la position des jetons dans une phrase. C'est la couche de `PositionalEmbedding` qui s'occupe de cela. À l'aide de la formule suivante, des plongements de position sont générés, ce qui permet d'incorporer la position d'un jeton dans son plongement : \n",
    "$$PE_{(pos, 2i)} = sin \\Big( \\frac{pos}{10000^{2i/d_{model}}} \\Big)$$\n",
    "$$PE_{(pos, 2i+1)} = cos \\Big( \\frac{pos}{10000^{2i/d_{model}}} \\Big)$$\n",
    "\n",
    "où $d_{model}$ est la dimension des plongements de sortie et $i$ est simplement l'indice d'une valeur dans le vecteur de plongement.\n",
    "\n",
    "La fonction `generate_positional_embedding` qui génère les plongements de position vous est fournie. Celle-ci prend en entrée :\n",
    "- `length` : Nombre de jetons maximal dont on doit générer le plongement de position\n",
    "- `depth` : Dimension des plongements du modèle.\n",
    "\n",
    "La fonction `call` de cette couche est appelée avec le paramètre suivant (les tailles des tenseurs sont indiquées entre parenthèses) :\n",
    "- `x` (de taille [batch_size, input_size] où le batch_size est le nombre d'éléments qui sont envoyés à la fois pour une itération de l'entraînement et input_size est la taille maximale des phrases en entrée) : Entrées de la couche. Cela correspond notamment au tenseur contenant les indices de chaque jeton correspondant à la phrase\n",
    "  \n",
    "\n",
    "Elle retourne le plongement de l'entrée dans l'espace latent incluant les positions des jetons (batch_size, input_size, dim_model). \n",
    "\n",
    "La fonction `call` doit effectuer les opérations suivantes :\n",
    "1. Appeler la couche `embedding_layer` qui génère des plongements par rapport aux entrées\n",
    "2. Multiplier chaque valeur par la racine de `dim_model` (Cette multiplication sert à agrandir les plongements pour qu'ils soient d'un ordre de grandeur comparable aux plongements de position qui sont ajoutés par la suite. Pour plus de détails, consultez l'article original ayant mené à la création du Transformer intitulé \"Attention Is All You Need\").\n",
    "3. Ajouter ensuite les plongements de positions aux plongements générés par la couche `embedding_layer` (après qu'ils aient été multipliés par la racine de `dim_model`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "czAWrftUdkvq"
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Classe représentant l'étape qui incorpore dans l'espace latent les positions des jetons\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, dim_model):\n",
    "        \"\"\"\n",
    "        Initialise une couche de plongements et les plongements de position\n",
    "\n",
    "        Args :\n",
    "            - input_size : Taille d'entrée de la couche (taille du vocabulaire)\n",
    "            - dim_model : Taille des plongements du modèle (taille du plongement de sortie de la couche)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding_layer = tf.keras.layers.Embedding(input_size, dim_model, mask_zero=True)\n",
    "        self.position_embeddings = self.generate_positions_embedding(length=2048, depth=dim_model)\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding_layer.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def generate_positions_embedding(self, length, depth):\n",
    "        depth = depth/2\n",
    "\n",
    "        positions = np.arange(length)[:, np.newaxis]\n",
    "        depths = np.arange(depth)[np.newaxis, :]/depth\n",
    "\n",
    "        angle_rates = 1 / (10000**depths)\n",
    "        angle_rads = positions * angle_rates\n",
    "\n",
    "        pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n",
    "\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Exécute la couche de plongements sur l'entrée en la normalisant sur la racine de la dimension de sortie\n",
    "        \"\"\"\n",
    "        x = self.embedding_layer(x) * tf.math.sqrt(tf.cast(self.dim_model, tf.float32))\n",
    "        x = x + self.position_embeddings[tf.newaxis, :tf.shape(x)[1], :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_OlPgAXYMJv"
   },
   "source": [
    "#### 4.2 Attention  (15 points)\n",
    "\n",
    "Les couches d'attention reposent toutes sur la même base qui contient une tête d'attention multiple, une couche de normalisation et une couche d'addition. La seule différence entre les différentes couches d'attention sont les entrées `Q` (query), `K` (key), et `V` (value) qui seront envoyées à la formule :\n",
    "\n",
    "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "Pour cela, la classe `DefaultAttention`, une classe dont toutes les autres couches d'attention hériteront, a été créée pour éviter de répéter 3 fois le même constructeur. Vous devez compléter les fonctions `call()` de chacune des sous-classes, soit `CrossAttention`, `GlobalSelfAttention` et `DecoderAttention`. Pour évaluer les valeurs de `K`, `V` et `Q` de chaque couche d'attention, référez-vous au graphique de l'architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Couche d'attention de base contenant des têtes d'attention suivies d'une couche de normalisation et d'addition\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.multiHeadAttention = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layerNormalization = tf.keras.layers.LayerNormalization()\n",
    "        self.addLayer = tf.keras.layers.Add()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 4.2.1 CrossAttention (5 points)\n",
    "Dans le cas de la couche `CrossAttention`, la fonction `call` prend en paramètres les entrées suivantes :\n",
    "- `input` : Les entrées de la couche, correspondant à la sortie de la couche `DecoderAttention`\n",
    "- `context` : La sortie de l'encodeur \n",
    "- `training` : Valeur booléenne indiquant si le modèle est en entraînement ou pas.\n",
    "\n",
    "Cette fonction doit exécuter les opérations suivantes :\n",
    "1. Appliquer la couche de têtes d'attention multiples avec les bonnes valeurs de `K`, `V` et `Q` (Ne pas oublier de passer l'argument `training` à la couche).\n",
    "2. Ajouter la sortie de la couche de tête d'attention aux entrées à l'aide de la couche `Add`\n",
    "3. Normaliser le tout à l'aide de la couche de normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(DefaultAttention):\n",
    "    \"\"\"\n",
    "    Couche qui connecte l'encodeur au décodeur.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialise une couche de têtes d'attention suivie d'une couche de normalisation \n",
    "        puis d'addition\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, input, context, training):\n",
    "        \"\"\"\n",
    "        Exécute la couche d'attention. Ajoute les sorties d'attention à l'entrée et \n",
    "        normalise le tout\n",
    "        \"\"\"\n",
    "        # TODO \n",
    "        Q = input\n",
    "        K = context\n",
    "        V = context\n",
    "        attention_output = self.multiHeadAttention(Q, K, V, training=training)\n",
    "        Q = self.addLayer([Q, attention_output])\n",
    "        Q = self.layerNormalization(Q)\n",
    "        return Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 4.2.2 GlobalSelfAttention  (5 points)\n",
    "Dans le cas de la couche `GlobalSelfAttention`, la fonction `call` prend en paramètres les entrée suivantes :\n",
    "- `input` : Les entrées de la couche, correspondant à la sortie de la couche `DecoderAttention`\n",
    "- `training` : Valeur booléenne indiquant si le modèle est en entraînement ou pas.\n",
    "\n",
    "Cette fonction doit exécuter les opérations suivantes :\n",
    "1. Appliquer la couche de têtes d'attention multiples avec les bonnes valeurs de `K`, `V` et `Q` (Ne pas oublier de passer l'argument `training` à la couche).\n",
    "2. Ajouter la sortie de la couche de tête d'attention aux entrées à l'aide de la couche `Add`\n",
    "3. Normaliser le tout à l'aide de la couche de normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalSelfAttention(DefaultAttention):\n",
    "    \"\"\"\n",
    "    Couch d'auto-attention permettant au modèle de regarder les autres mots de \n",
    "    la phrase d'entrée lorsqu'il encode un mot spécifique\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialise une couche de têtes d'attention suivie d'une couche de \n",
    "        normalisation puis d'addition\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, input, training):\n",
    "        \"\"\"\n",
    "        Exécute la couche d'attention. Ajoute les sorties d'attention à l'entrée \n",
    "        et normalise le tout\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        Q, K, V = input, input, input\n",
    "        attention_output = self.multiHeadAttention(Q, K, V, training=training)\n",
    "        Q = self.addLayer([Q, attention_output])\n",
    "        Q = self.layerNormalization(Q)\n",
    "        return Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.3 DecoderAttention  (5 points)\n",
    "Dans le cas de la couche `DecoderAttention`, la fonction `call` prend en paramètres les entrées suivantes :\n",
    "- `input` : Les entrées de la couche, correspondant à la sortie de la couche `DecoderAttention`\n",
    "- `training` : Valeur booléenne indiquant si le modèle est en entraînement ou pas.\n",
    "\n",
    "L'implémentation de la méthode est très similaire à la fonction `call` de la classe `GlobalSelfAttention`, mais diffère en un point clé : le masque causal. Ce masque permet notamment de ne pas considérer les jetons futurs lorsque le mécanisme d'attention est calculé. Cela évite au Transformer de s'entraîner en connaissant les jetons futurs qu'il doit prédire (donc en \"trichant\"). Cet [article](https://medium.com/analytics-vidhya/masking-in-transformers-self-attention-mechanism-bad3c9ec235c) donne plus d'information sur le masque causal.\n",
    "\n",
    "Cette fonction doit exécuter les opérations suivantes :\n",
    "1. Appliquer la couche de têtes d'attention multiples avec les bonnes valeurs de `K`, `V` et `Q` (Ne pas oublier de passer l'argument `training` à la couche et d'activer le masque causal de la couche en mettant l'attribut `use_causal_mask` à `True` lors de l'appel de la couche d'attention).\n",
    "2. Ajouter la sortie de la couche de tête d'attention aux entrées à l'aide de la couche `Add`\n",
    "3. Normaliser le tout à l'aide de la couche de normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "o7UdIeC7XBUz"
   },
   "outputs": [],
   "source": [
    "class DecoderAttention(DefaultAttention):\n",
    "    \"\"\"\n",
    "    Couche d'attention semblable à la couche globale d'auto-attention, mais en masquant \n",
    "    les données qui viennent après\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialise une couche de têtes d'attention suivie d'une couche de normalisation \n",
    "        puis d'addition\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, input, training):\n",
    "        \"\"\"\n",
    "        Exécute la couche d'attention en masquant les données après. Ajoute les sorties \n",
    "        d'attention à l'entrée et normalise le tout\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        Q, K, V = input, input, input\n",
    "        attention_output = self.multiHeadAttention(Q, K, V, training=training, use_causal_mask=True)\n",
    "        Q = self.addLayer([Q, attention_output])\n",
    "        Q = self.layerNormalization(Q)\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez tester votre implémentation des couches d'attention à l'aide de la fonction suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Attention result : \n",
      "tf.Tensor(\n",
      "[[[ 124 -119   -4]]\n",
      "\n",
      " [[ 140  -59  -80]]\n",
      "\n",
      " [[  79   60 -140]]], shape=(3, 1, 3), dtype=int32) \n",
      "\n",
      "Global-Self Attention result : \n",
      "tf.Tensor(\n",
      "[[[ 124 -119   -4]]\n",
      "\n",
      " [[ 140  -59  -80]]\n",
      "\n",
      " [[  79   60 -140]]], shape=(3, 1, 3), dtype=int32) \n",
      "\n",
      "Decoder Attention result : \n",
      "tf.Tensor(\n",
      "[[[ 124 -119   -4]]\n",
      "\n",
      " [[ 140  -59  -80]]\n",
      "\n",
      " [[  79   60 -140]]], shape=(3, 1, 3), dtype=int32) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_attention():\n",
    "    config = {\n",
    "        'num_heads': 3,\n",
    "        'key_dim': 3,\n",
    "        'dropout': 0.1\n",
    "    }\n",
    "    cross_attention = CrossAttention(**config)\n",
    "    global_self_attention = GlobalSelfAttention(**config)\n",
    "    decoder_attention = DecoderAttention(**config)\n",
    "\n",
    "    # Create determinisitc inputs and context\n",
    "    generator = tf.random.Generator.from_seed(1)\n",
    "    input = generator.normal(shape=(3, 1, 3))\n",
    "    context = generator.normal(shape=(3, 1, 3))\n",
    "\n",
    "    # Make attention layer deterministic\n",
    "    layer = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=4, dropout=0.1, kernel_initializer=tf.keras.initializers.ones())\n",
    "    cross_attention.multiHeadAttention = layer\n",
    "    global_self_attention.multiHeadAttention = layer\n",
    "    decoder_attention.multiHeadAttention = layer\n",
    "\n",
    "    outputs_cross_attention = tf.cast(cross_attention(input, context) * 100, tf.int32)\n",
    "    outputs_global_self_attention = tf.cast(global_self_attention(input) * 100, tf.int32)\n",
    "    outputs_decoder_attention = tf.cast(decoder_attention(input) * 100, tf.int32)\n",
    "\n",
    "    print('Cross Attention result : ')\n",
    "    print(outputs_cross_attention, '\\n')\n",
    "\n",
    "    print('Global-Self Attention result : ')\n",
    "    print(outputs_global_self_attention, '\\n')\n",
    "\n",
    "    print('Decoder Attention result : ')\n",
    "    print(outputs_decoder_attention, '\\n')\n",
    "\n",
    "test_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sortie attendue : \n",
    "\n",
    "```\n",
    "Cross Attention result : \n",
    "tf.Tensor(\n",
    "[[[ 124 -119   -4]]\n",
    "\n",
    " [[ 140  -59  -80]]\n",
    "\n",
    " [[  79   60 -140]]], shape=(3, 1, 3), dtype=int32) \n",
    "\n",
    "Global-Self Attention result : \n",
    "tf.Tensor(\n",
    "[[[ 124 -119   -4]]\n",
    "\n",
    " [[ 140  -59  -80]]\n",
    "\n",
    " [[  79   60 -140]]], shape=(3, 1, 3), dtype=int32) \n",
    "\n",
    "Decoder Attention result : \n",
    "tf.Tensor(\n",
    "[[[ 124 -119   -4]]\n",
    "\n",
    " [[ 140  -59  -80]]\n",
    "\n",
    " [[  79   60 -140]]], shape=(3, 1, 3), dtype=int32) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84vLhOVGY7vf"
   },
   "source": [
    "#### 4.3 Feed Forward  (5 points)\n",
    "\n",
    "La couche Feed Forward est, dans notre cas, simplement une séquence de 2 couches denses, d'une couche de dropout, d'une couche d'addition et d'une couche de normalisation. Ces couches sont déjà initialisées dans le constructeur à l'aide d'une couche `Sequential` qui regroupe plusieurs couches et les applique une à la suite de l'autre.\n",
    "\n",
    "La fonction `call` prend en paramètres les entrées suivantes :\n",
    "- `input` : Entrées de la couche (varie en fonction d'où est située cette couche dans l'architecture)\n",
    "\n",
    "Elle retourne ensuite le résultat une fois que les transformations sont appliquées sur les entrées\n",
    "\n",
    "Elle effectue les opérations suivantes :\n",
    "1. Exécute la couche séquentielle initialisée dans le constructeur\n",
    "2. Ajoute le résultat de la couche séquentielle aux entrées\n",
    "3. Normalise le tout à l'aide de la couche de normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "gm_4ethRYKR_"
   },
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Couche de propagation à la sortie des couches d'attention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim_model, feed_forward_size, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialise des couches de propagation dense (avec dropout), d'addition et de normalisation\n",
    "        Args :\n",
    "            - dim_model : Dimension du modèle (sortie de la couche)\n",
    "            - feed_forward_size : Dimension de la couche dense de propagation (entrée)\n",
    "            - dropout_rate : Ratio des entrées de la couche de dropout qui \n",
    "            seront initialisés à zéro de manière aléatoire\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(feed_forward_size, activation='relu'),\n",
    "            tf.keras.layers.Dense(dim_model),\n",
    "            tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, input):\n",
    "        \"\"\"\n",
    "        Exécute les couches de propagation sur l'entrée, additionne le tout et normalise\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        input = self.add([input, self.seq(input)])\n",
    "        input = self.layer_norm(input)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxPcLmhrZq-c"
   },
   "source": [
    "#### 4.4 Encodeur (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'encodeur de notre Transformer est composé en réalité de plusieurs couches appelées `EncoderLayer`. Ces couches représentent une seule passe d'un encodeur. Cependant, la classe `Encoder` regroupe plusieurs de ces `EncoderLayer` pour permettre au Transformer de capturer des contextes plus compliqués entre les mots.\n",
    "\n",
    "Vous aurez donc à compléter la méthode `call` de la classe `EncoderLayer`. Cette méthode prend en entrée les paramètres suivants :\n",
    "- `input` : Entrées de la couche (notamment la sortie de la classe `PositionalEmbedding`)\n",
    "- `training` : Valeur booléenne indiquant si la méthode est appelée durant l'entraînement ou pas\n",
    "\n",
    "Elle retourne les entrées une fois qu'elles sont passées à travers toutes les couches (`GlobalSelfAttention`, `FeedForward`)\n",
    "\n",
    "Cette méthode devra exécuter les opérations suivantes :\n",
    "1. Appeler la couche d'attention avec les entrées\n",
    "2. Appeler la couche de propagation sur la sortie de la couche d'attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "IIyENqA5Y4za"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Classe représentant une couche d'encodeur\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *, dim_model, num_heads, feed_forward_size, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialise une couche d'auto-attention suivie d'une couche de propagation\n",
    "\n",
    "        Args :\n",
    "            dim_model : Dimension des embeddings du model\n",
    "            num_heads : Nombre de têtes d'attention de l'encodeur\n",
    "            feed_forward_size : Nombre de neurones du feed forward\n",
    "            dropout_rate : Ratio des entrées de la couche d'attention qui seront \n",
    "            initialisés à zéro de manière aléatoire\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=dim_model,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.ffn = FeedForward(dim_model, feed_forward_size)\n",
    "\n",
    "    def call(self, input, training):\n",
    "        \"\"\"\n",
    "        Exécute la couche d'attention et de propagation sur les entrées.\n",
    "        L'argument training spécifie si l'appel est effectué durant l'entrainement \n",
    "        ou pas (important pour la couche d'attention)\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        input = self.self_attention(input, training=training)\n",
    "        input = self.ffn(input)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, la classe `Encoder` s'occupe de regrouper plusieurs `EncoderLayer` pour permettre au Transformer d'inférer des contextes plus complexes.\n",
    "\n",
    "La méthode `call` de la classe `Encoder` prend en entrée les paramètres suivants :\n",
    "- `input` : Entrées de la couche (correspondant aux indices des jetons de la phrase)\n",
    "- `training` : Valeur booléenne indiquant si la méthode est appelée durant l'entraînement ou pas\n",
    "\n",
    "Elle retourne les entrées une fois qu'elles sont passées à travers toutes les couches d'encodeur\n",
    "\n",
    "Cette méthode exécute les opérations suivantes :\n",
    "1. Appeler la couche de plongements de position sur les entrées\n",
    "2. Appliquer la couche de dropout sur le résultat\n",
    "3. Appeler toutes les couches `EncoderLayer` (la sortie d'une couche d'encodeur devient l'entrée d'une autre)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "OoIdsp__ZjkE"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Classe représentant tous les encodeurs du Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *, num_layers, dim_model, num_heads, feed_forward_size, vocab_size, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialise la couche de plongements de position, une couche dropout et les couches d'encodeurs\n",
    "        Args :\n",
    "            num_layers : Nombre de couches d'encodeurs\n",
    "            dim_model : Dimension des embeddings du model\n",
    "            num_heads : Nombre de têtes d'attention de l'encodeur\n",
    "            feed_forward_size : Dimension du feed forward (en sortie)\n",
    "            vocab_size : Taille du vocabulaire (correspondant à la taille d'entrée de la \n",
    "            couche de plongements de position)\n",
    "            dropout_rate : Ratio des entrées de la couche de dropout qui seront initialisés \n",
    "            à zéro de manière aléatoire\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim_model = dim_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(input_size=vocab_size, dim_model=dim_model)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.enc_layers = [EncoderLayer(dim_model=dim_model, num_heads=num_heads, feed_forward_size=feed_forward_size, dropout_rate=dropout_rate) for _ in range(num_layers)]\n",
    "\n",
    "    def call(self, input, training):\n",
    "        \"\"\"\n",
    "        Execute la couche de plongements et de dropout puis toutes les couches d'encodeurs\n",
    "        \"\"\"\n",
    "        input = self.dropout(self.pos_embedding(input))\n",
    "        for i in range(self.num_layers):\n",
    "            input = self.enc_layers[i](input, training)\n",
    "\n",
    "        return input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gL0Nvfwcf1x"
   },
   "source": [
    "#### 4.5 Decodeur (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le décodeur de notre Transformer est composé en réalité de plusieurs couches appelées `DecoderLayer`. Ces couches représentent une seule passe d'un décodeur. Cependant, la classe `Decoder` regroupe plusieurs de ces `DecoderLayer` pour permettre au Transformer de capturer des contextes plus compliqués entre les mots.\n",
    "\n",
    "Vous aurez donc à compléter la méthode `call` de la classe `DecoderLayer`. Cette méthode prend en entrée les paramètres suivants :\n",
    "- `input` : Entrées de la couche\n",
    "- `context` : Le contexte des couches d'attention\n",
    "- `training` : Valeur booléenne indiquant si la méthode est appelée durant l'entraînement ou pas\n",
    " \n",
    "Elle retourne les entrées une fois qu'elles sont passées à travers toutes les couches (`DecoderAttention`, `CrossAttention`, `FeedForward`)\n",
    "\n",
    "Cette méthode devra exécuter les opérations suivantes :\n",
    "1. Appeler la couche d'attention du décodeur avec les entrées\n",
    "2. Appeler la couche d'attention croisée\n",
    "3. Appeler la couche de propagation (`FeedForward`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "7t3-gOticXIr"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Classe représentant une couche de décodeur\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *, dim_model, num_heads, feed_forward_size, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Args :\n",
    "            dim_model : Dimension des embeddings du model\n",
    "            num_heads : Nombre de têtes d'attention du décodeur\n",
    "            feed_forward_size : Nombre de neurones du feed forward\n",
    "            dropout_rate : Ratio de dropout pour les neurones de la couche de Feed Forward\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_decoder_attention = DecoderAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=dim_model,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.cross_attention = CrossAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=dim_model,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.ffn = FeedForward(dim_model, feed_forward_size)\n",
    "\n",
    "    def call(self, input, context, training):\n",
    "        \"\"\"\n",
    "        Exécute les couches d'attention suivies des couches de propagation FFN\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        input = self.encoder_decoder_attention(input, training=training)\n",
    "        input = self.cross_attention(input, context, training=training)\n",
    "        input = self.ffn(input)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classe `Decoder` s'occupe de regrouper plusieurs `DecoderLayer`.\n",
    "\n",
    "La méthode `call` de la classe `Decoder`prend en entrée les paramètres suivants :\n",
    "- `input` : Entrées de la couche (correspondant aux indices des jetons de la phrase)\n",
    "- `context` : Contexte des couches d'attention (correspondant à la sortie de l'encodeur)\n",
    "- `training` : Valeur booléenne indiquant si la méthode est appelée durant l'entraînement ou pas\n",
    "\n",
    "Elle retourne les entrées une fois qu'elles sont passées à travers toutes les couches de décodeur\n",
    "\n",
    "Cette méthode  exécute les opérations suivantes :\n",
    "1. Appeler la couche de plongements de position sur les entrées\n",
    "2. Appliquer la couche de dropout sur le résultat\n",
    "3. Appeler les couches `DecoderLayer` successivement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "NdzjzJ2wcwDu"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, dim_model, num_heads, feed_forward_size, vocab_size, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialise la couche de plongements de position, une couche dropout et les couches d'encodeurs\n",
    "        Args :\n",
    "            num_layers : Nombre de couches de décodeur\n",
    "            dim_model : Dimension des embeddings du model\n",
    "            num_heads : Nombre de têtes d'attention de l'encodeur\n",
    "            feed_forward_size : Dimension du feed forward (en sortie)\n",
    "            vocab_size : Taille du vocabulaire (correspondant à la taille d'entrée \n",
    "            de la couche de plongements de position)\n",
    "            dropout_rate : Ratio des entrées de la couche de dropout qui seront \n",
    "            initialisés à zéro de manière aléatoire\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim_model = dim_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(input_size=vocab_size, dim_model=dim_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dec_layers = [DecoderLayer(dim_model=dim_model, num_heads=num_heads, feed_forward_size=feed_forward_size, dropout_rate=dropout_rate) for x in range(self.num_layers)]\n",
    "\n",
    "        self.last_attn_scores = None\n",
    "\n",
    "    def call(self, input, context, training):\n",
    "        \"\"\"\n",
    "        Execute la couche de plongements et de dropout \n",
    "        puis toutes les couches de décodeurs\n",
    "        \"\"\"\n",
    "        input = self.dropout(self.pos_embedding(input))\n",
    "        for i in range(self.num_layers):\n",
    "            input = self.dec_layers[i](input, context, training)\n",
    "\n",
    "        return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVZN8skQdkvp"
   },
   "source": [
    "#### 4.6 Transformer (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le Transformer est maintenant prêt à être créé. Le constructeur s'occupe déjà d'initialiser tous les attributs nécessaires à son fonctionnement.\n",
    "\n",
    "La fonction `call` prend en entrées les arguments suivants :\n",
    "- `inputs` : Les entrées du modèle de la forme d'un tuple regroupant l'entrée sparql et l'entrée anglaise (`inputs = (sparql, english)`)\n",
    "- `training` : Valeur booléenne indiquant si le modèle est en entrainement ou pas\n",
    "\n",
    "La méthode `call` doit :\n",
    "1. Séparer les entrées reçues en sparql et english\n",
    "2. Envoyer les phrases sparql à l'encodeur\n",
    "3. Envoyer les phrases en anglais au décodeur avec comme contexte la sortie de l'encodeur\n",
    "4. Envoyer la sortie du décodeur à la couche dense initialisée dans le constructeur (`self.dense_layer`)\n",
    "5. Appeler la fonction `drop_mask` avec comme argument les probabilités générées par la couche dense (en enlevant l'attribut `_keras_mask` des probabilités générées par la couche dense, on évite au modèle d'utiliser ce masque lorsqu'il calcule les métriques et le coût)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "NMNM6hDldKs6"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Classe représentant le Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *, num_layers, dim_model, num_heads, feed_forward_size,\n",
    "                input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialise les couches d'encodeur et de décodeurs et la couche dense finale\n",
    "        Args :\n",
    "            num_layers : Nombre de couches de décodeur\n",
    "            dim_model : Dimension des embeddings du model\n",
    "            num_heads : Nombre de têtes d'attention de l'encodeur et du décodeur\n",
    "            feed_forward_size : Dimension du feed forward (en sortie)\n",
    "            input_vocab_size : Taille du vocabulaire d'entrée\n",
    "            target_vocab_Size : Taille du vocabulaire de sortie\n",
    "            dropout_rate : Ratio des entrées de la couche de dropout qui seront \n",
    "            initialisés à zéro de manière aléatoire\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers=num_layers, dim_model=dim_model,\n",
    "                            num_heads=num_heads, feed_forward_size=feed_forward_size,\n",
    "                            vocab_size=input_vocab_size,\n",
    "                            dropout_rate=dropout_rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, dim_model=dim_model,\n",
    "                            num_heads=num_heads, feed_forward_size=feed_forward_size,\n",
    "                            vocab_size=target_vocab_size,\n",
    "                            dropout_rate=dropout_rate)\n",
    "\n",
    "        self.dense_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        \"\"\"\n",
    "        Appel les couches d'encodeur et de décodeur avec les bonnes entrées et \n",
    "        contexte ainsi que la couche dense finale\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        sparql, english = inputs\n",
    "        sparql = self.encoder(sparql, training=training)\n",
    "        english = self.decoder(english, sparql, training=training)\n",
    "        english = self.dense_layer(english)\n",
    "        self.drop_mask(training, english)\n",
    "        return english\n",
    "\n",
    "    def drop_mask(self, training, probabilities):\n",
    "        if not training:\n",
    "            try:\n",
    "                del probabilities._keras_mask\n",
    "            except AttributeError:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez tester votre implémentation finale du Transformer avec la fonction suivante. **Attention, ce n'est pas parce que vous obtenez les bons résultats qu'il n'y a pas de bugs dans votre implémentation, mais c'est déjà un bon signe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[ 0.00026987 -0.03291528]\n",
      "   [ 0.00026987 -0.03291528]]\n",
      "\n",
      "  [[ 0.00026987 -0.03291528]\n",
      "   [ 0.00026987 -0.03291528]]]\n",
      "\n",
      "\n",
      " [[[ 0.00026988 -0.03291521]\n",
      "   [ 0.00026984 -0.03291551]]\n",
      "\n",
      "  [[ 0.00026987 -0.03291528]\n",
      "   [ 0.00026988 -0.03291521]]]], shape=(2, 2, 2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def test_transformer():\n",
    "\n",
    "    config = {\n",
    "        'num_layers': 2,\n",
    "        'dim_model': 2,\n",
    "        'num_heads': 2,\n",
    "        'feed_forward_size': 2,\n",
    "        'input_vocab_size': 2,\n",
    "        'target_vocab_size': 2,\n",
    "        'dropout_rate': 0.1\n",
    "    }\n",
    "\n",
    "    initializer = tf.keras.initializers.glorot_normal(42)\n",
    "\n",
    "    feed_forward = FeedForward(2, 2, 0.1)\n",
    "    feed_forward.seq = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(2, activation='relu', kernel_initializer=initializer, use_bias=False),\n",
    "        tf.keras.layers.Dense(2, kernel_initializer=initializer, use_bias=False),\n",
    "        tf.keras.layers.Dropout(0.1, seed=42)\n",
    "    ])\n",
    "    feed_forward.add = tf.keras.layers.Add()\n",
    "    feed_forward.layer_norm = tf.keras.layers.LayerNormalization(beta_initializer=initializer, gamma_initializer=initializer)\n",
    "\n",
    "    transformer = Transformer(**config)\n",
    "\n",
    "    transformer.encoder.pos_embedding.embedding_layer = tf.keras.layers.Embedding(2, 2, embeddings_initializer=initializer, mask_zero=False)\n",
    "    for l in transformer.encoder.enc_layers:\n",
    "        l.self_attention = GlobalSelfAttention(num_heads=2, key_dim=2, dropout=0.1, kernel_initializer=initializer)\n",
    "        l.ffn = feed_forward\n",
    "    transformer.encoder.dropout = tf.keras.layers.Dropout(0.1, seed=42)\n",
    "\n",
    "    transformer.decoder.pos_embedding.embedding_layer = tf.keras.layers.Embedding(2, 2, embeddings_initializer=initializer, mask_zero=True)\n",
    "    for l in transformer.decoder.dec_layers:\n",
    "        l.cross_attention = CrossAttention(num_heads=2, key_dim=2, dropout=0.1, kernel_initializer=initializer)\n",
    "        l.encoder_decoder_attention = DecoderAttention(num_heads=2, key_dim=2, dropout=0.1, kernel_initializer=initializer)\n",
    "        l.ffn = feed_forward\n",
    "        \n",
    "    transformer.dense_layer = tf.keras.layers.Dense(2, kernel_initializer=initializer, use_bias=False)\n",
    "    transformer.decoder.dropout = tf.keras.layers.Dropout(0.1, seed=42)\n",
    "\n",
    "    # Create determinisitc inputs and context\n",
    "    generator = tf.random.Generator.from_seed(1)\n",
    "    input = generator.normal(shape=(2, 2, 2))\n",
    "    context = generator.normal(shape=(2, 2, 2))\n",
    "\n",
    "    input_transformer = (input, context)\n",
    "    output = transformer(input_transformer, training=True)\n",
    "    print(output)\n",
    "\n",
    "test_transformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.Tensor(\n",
    "[[[[ 0.00026983 -0.03291529]\n",
    "   [ 0.00026987 -0.03291498]]\n",
    "\n",
    "  [[ 0.00026987 -0.03291498]\n",
    "   [ 0.00026987 -0.03291498]]]\n",
    "\n",
    "\n",
    " [[[ 0.00026983 -0.03291529]\n",
    "   [ 0.00026987 -0.03291498]]\n",
    "\n",
    "  [[ 0.00026987 -0.03291498]\n",
    "   [ 0.00026987 -0.03291498]]]], shape=(2, 2, 2, 2), dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCaWH9jCd51q"
   },
   "source": [
    "#### 4.7 Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classe `Scheduler` permet entre autre de mettre à jour le taux d'apprentissage du modèle lors de l'entraînement. Son implémentation complète vous est fournie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "R7KODnRNdeHW"
   },
   "outputs": [],
   "source": [
    "class Scheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, dim_model, warmup_steps):\n",
    "        super().__init__()\n",
    "        self.dim_model = tf.cast(dim_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        return tf.math.rsqrt(self.dim_model) * tf.math.minimum(tf.math.rsqrt(step), step * (self.warmup_steps ** -1.5))\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'd_model': self.dim_model,\n",
    "            'warmup_steps': self.warmup_steps,\n",
    "        }\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-wLAeNReP_s"
   },
   "source": [
    "### 5. Entrainement du modèle (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est maintenant temps de créer le traducteur qui va transformer des requêtes sparql en anglais. Pour cela, il faudra compléter 4 méthodes de la classe `Translator`, soit les méthodes `prepare`, `fit` et `translate`.\n",
    "___\n",
    "\n",
    "La fonction `prepare` reçoit les données d'entrainement et de validation sous forme de pandas DataFrame et s'occupe de :\n",
    "1. Appeler le préprocesseur sur les données d'entrainement et de validation\n",
    "2. Créer un objet `tf.Dataset` contenant un tuple des requêtes sparql et des questions en anglais pour l'ensemble d'entraînement et de validation\n",
    "3. Envoyer les 2 datasets créés (entraînement et validation) au batcher\n",
    "\n",
    "Elle retourne un tuple contenant les batches d'entraînement et de validation\n",
    "\n",
    "___\n",
    "\n",
    "La fonction `fit` s'occupe simplement d'entraîner le modèle avec les données d'entraînement et de validation passés en paramètre.\n",
    "___\n",
    "\n",
    "La fonction `translate` s'occupe de traduire une série de données sparql en anglais. Pour cela, plusieurs étapes doivent être effectuées. Elle doit :\n",
    "1. Appliquer le préprocesseur sur l'ensemble de test donné\n",
    "2. Créer des batches à l'aide du batcher de test\n",
    "3. Pour chaque valeur dans les batches créés\n",
    "  - Extraire le contenu du tuple. Souvenez-vous que ce qui est ressorti par la méthode `prepare_batch` dans le cas d'un batcher de test est une tuple de la forme (phrase SPARQL, phrase anglais) où initialement, la phrase anglaise est initialisée avec le jeton de départ\n",
    "  - Envoyer les contextes et les phrases au Transformer pour qu'il prédise le prochain jeton\n",
    "  - Concaténer ensemble tous les jetons prédits par le Transformer pour générer la traduction  \n",
    "4. Réduire la taille des prédictions pour enlever tout ce qui vient après le jeton de fin généré par le Transformer (si aucun jeton de fin n'est généré, la traduction n'a pas besoin d'être coupée)\n",
    "5. Transformer les jetons prédits en mots à l'aide du bon tokenizer\n",
    "6. Annuler les transformations initiales effectuées à l'aide du pré-traitement\n",
    "\n",
    "\n",
    "Les fonctions `masked_loss` et `masked_accuracy` vous sont fournies et permettent d'évaluer l'exactitude du Transformer en évaluant une fonction de perte propre au Transformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_Ta5Z0YfnA1"
   },
   "outputs": [],
   "source": [
    "class Translator:\n",
    "\n",
    "    num_layers = 4\n",
    "    dim_model = 128\n",
    "    feed_forward_size = 512\n",
    "    num_heads = 6\n",
    "    dropout_rate = 0.1\n",
    "    input_vocab_size = 8000\n",
    "    target_vocab_size = 8000\n",
    "    batch_size = 64\n",
    "    batch_size_test = 500\n",
    "    buffer_size = 20000\n",
    "    buffer_size_test = None\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialise le preprocessor, les tokenizers, les batchers et le Transformer \n",
    "        avec les bons paramètres\n",
    "        \"\"\"\n",
    "\n",
    "        self.pre_processor = Preprocessor()\n",
    "\n",
    "        self.tokenizers = GroupedTokenizers(\n",
    "            LanguageTokenizer.reserved_tokens,\n",
    "            root + 'language_vocab_english.txt',\n",
    "            root + 'language_vocab_sparql.txt'\n",
    "        )\n",
    "\n",
    "        self.train_batcher = Batcher(tokenizers=self.tokenizers, train=True, max_tokens=Translator.dim_model, batch_size=Translator.batch_size, buffer_size=Translator.buffer_size)\n",
    "        self.test_batcher = Batcher(tokenizers=self.tokenizers, train=False, max_tokens=Translator.dim_model, batch_size=Translator.batch_size_test, buffer_size=Translator.buffer_size_test)\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            num_layers=Translator.num_layers,\n",
    "            dim_model=Translator.dim_model,\n",
    "            num_heads=Translator.num_heads,\n",
    "            feed_forward_size=Translator.feed_forward_size,\n",
    "            input_vocab_size=Translator.input_vocab_size,\n",
    "            target_vocab_size=Translator.target_vocab_size,\n",
    "            dropout_rate=Translator.dropout_rate)\n",
    "\n",
    "        self.scheduler = Scheduler(Translator.dim_model, 4000)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(self.scheduler, beta_1=0.9, beta_2=0.95, epsilon=1e-9)\n",
    "\n",
    "        self.transformer.compile(\n",
    "            loss=Translator.masked_loss,\n",
    "            optimizer=self.optimizer,\n",
    "            metrics=[Translator.masked_accuracy])\n",
    "\n",
    "        self.end = self.tokenizers.sparql.tokenize([''])[0][1][tf.newaxis]\n",
    "\n",
    "    def prepare(self, train: pd.DataFrame, val: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Prépare les ensembles de validation et d'entrainement à l'entrainement \n",
    "        en les envoyant au preprocessor et au batcher\n",
    "        Args :\n",
    "            - train : DataFrame d'entrainement avec les columns sparql (entrée) et anglais (sortie)\n",
    "            - val : DataFrame de validation avec les columns sparql (entrée) et anglais (sortie)\n",
    "\n",
    "        Returns :\n",
    "            Tuple contenant les batches d'entraînement et les batches de validation\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def fit(self, training, validation, epochs=50):\n",
    "        \"\"\"\n",
    "        Entraine le modèle en utilisant l'ensemble d'entrainement et valide le résultat\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def translate(self, sparql: pd.Series):\n",
    "        \"\"\"\n",
    "        Traduit une série de requêtes sparql en anglais\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def masked_loss(label, pred):\n",
    "        mask = label != 0\n",
    "        loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction='none')\n",
    "        loss = loss_object(label, pred)\n",
    "\n",
    "        mask = tf.cast(mask, dtype=loss.dtype)\n",
    "        loss *= mask\n",
    "\n",
    "        loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "        return loss\n",
    "\n",
    "    def masked_accuracy(label, pred):\n",
    "        pred = tf.argmax(pred, axis=2)\n",
    "        label = tf.cast(label, pred.dtype)\n",
    "        match = label == pred\n",
    "\n",
    "        mask = label != 0\n",
    "\n",
    "        match = match & mask\n",
    "\n",
    "        match = tf.cast(match, dtype=tf.float32)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Préparation des données \n",
    "\n",
    "Exécuter ensuite la cellule ci-dessous pour créer maintenant une instance de la classe `Translator`, charger les données d'entraînement et de validation et préparer les données à l'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPVlAR77eRlt"
   },
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    training_path=root + 'train.csv',\n",
    "    validation_path=root + 'validation.csv'\n",
    ")\n",
    "\n",
    "train_batch, val_batch = translator.prepare(data_loader.train, data_loader.val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Entraînement \n",
    "\n",
    "Entraînez le modèle avec les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1u45C28kr0vS",
    "outputId": "465a31a1-2222-49d4-9c6f-cef0fa2fb1d2"
   },
   "outputs": [],
   "source": [
    "translator.fit(train_batch, val_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Traduction \n",
    "\n",
    "Effectuez la traduction des données de test pour valider l'efficacité du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aLcN1y9GLqwH",
    "outputId": "3247c5db-88eb-4df7-a5b9-b9f21155b455"
   },
   "outputs": [],
   "source": [
    "predictions = translator.translate(data_loader.val['sparql'])\n",
    "formatted_predictions = pd.concat([pd.DataFrame(predictions), data_loader.val], axis=1)\n",
    "formatted_predictions.drop(['id', 'sparql'], inplace=True, axis=1)\n",
    "formatted_predictions.rename(columns={0:'prediction', 'english':'target_text'}, inplace=True)\n",
    "formatted_predictions.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZSxn1aTholW"
   },
   "source": [
    "### 6. Évalution : Métrique BLEU (15 points)\n",
    "\n",
    "Pour évaluer l'efficacité des traductions, la métrique BLEU sera utilisée. La formule est donnée ci-dessous : \n",
    "$$BLEU = BP * exp \\Big( \\sum_{n=1}^{N} w_n log p_n \\Big)$$\n",
    "\n",
    "où $p_n$ est la précision modifiée pour le n-gramme (correspondant au ratio de la fréquence maximum du n-gramme dans chaque phrase de référence par la fréquence du n-gramme). Posons ensuite $r$ comme le nombre de mots dans la phrase cible et $c$ comme le nombre de mots dans la phrase prédite. Si $c>r$, alors BP vaut 1. Sinon $BP = exp(1 - \\frac{r}{c})$.\n",
    "\n",
    "Les valeurs des poids $w_n$ est ce qui donne les différentes variations de la métrique BLEU. Dans notre cas, la métrique BLEU-3 sera utilisée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EoCj2WfhiMKW"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Évalue la précision du modèle en utilisant la métrique BLEU\n",
    "    Args :\n",
    "        - data : DataFrame contenant deux colonnes (predictions et target_text)\n",
    "\n",
    "    Returns :\n",
    "        La moyenne du score BLEU\n",
    "    \"\"\"\n",
    "    weights = (1/3, 1/3, 1/3) # Use Bleu-3\n",
    "    scores = np.zeros(data.shape[0])\n",
    "    index = 0\n",
    "    for iter, row in data.iterrows():\n",
    "        if not pd.notnull(row['prediction']):\n",
    "            continue\n",
    "        prediction = row['prediction'].split()\n",
    "        target_text = row['target_text'].split()\n",
    "\n",
    "        scores[index] = sentence_bleu([target_text], prediction, weights=weights)\n",
    "\n",
    "        index += 1\n",
    "    return np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Évaluation du modèle \n",
    "\n",
    "Appelez la fonction `evaluate_model` sur les prédictions de votre modèle pour évaluer sa performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aZVLOVd2wSfd",
    "outputId": "b4ef3219-ae07-4c54-bef3-1e8fd4e314b5"
   },
   "outputs": [],
   "source": [
    "evaluate_model(formatted_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. Analyse des erreurs (10 points)\n",
    "Analysez les traductions du modèle et ses erreurs. Implantez une analyse statistique  (selon la forme de votre choix) qui affiche des catégories d'erreurs et leur % d'occurrence parmi l'ensemble des erreurs possibles. Vous pouvez orienter votre fonction pour qu'elle décrive des dimensions spécifiques. Par exemple : les erreurs sont-elles plus souvent sur les éléments de la base de connaissance \"dbx\" ou sur le reste des jetons ? Les erreurs sont-elles dues à des éléments qui ne sont pas vus en entrainement ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Amélioration (5 points)\n",
    "Donnez des pistes de solution pour améliorer le score BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIVRABLES:\n",
    "Vous devez remettre sur Moodle un zip contenant les fichiers suivants :\n",
    "\n",
    "1-\tLe code : Vous devez compléter le squelette inf8460_tp3.ipynb sous le nom   equipe_i_inf8460_TP3.ipynb (i = votre numéro d’équipe). Indiquez vos noms et matricules au début du notebook. Ce notebook doit contenir les fonctionnalités requises avec des commentaires appropriés. Le code doit être exécutable sans erreur et accompagné de commentaires appropriés de manière à expliquer les différentes fonctions. Les critères de qualité tels que la lisibilité du code et des commentaires sont importants. Tout votre code et vos résultats doivent être exécutables et reproductibles ; \n",
    "\n",
    "2-\tUn fichier pdf représentant votre notebook complètement exécuté sous format pdf. \n",
    "Pour créer le fichier cliquez sur File > Download as > PDF via LaTeX (.pdf). Assurez-vous que le PDF est entièrement lisible.\n",
    "\n",
    "\n",
    "## EVALUATION \n",
    "Votre TP sera évalué selon les critères suivants :\n",
    "\n",
    "1. Exécution correcte du code\n",
    "2. Qualité du code (noms significatifs, structure, performance, gestion d’exception, etc.)\n",
    "3. Commentaires clairs et informatifs\n",
    "4. Performance attendue des modèles\n",
    "5. Réponses correctes/sensées aux questions de réflexion ou d'analyse\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "history_visible": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "71122587d5da4615a3ae38c5f340d616": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "717f3c33a56b40428ce424fe866813ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b41171ae6d2e497ab994f317b15210fd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7b450e0325e42f7b57ce38c54a899f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c117cdc210234b74a6ee5a1698062f56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e5ec1aa87c02490bb124209a6a0b9d58",
      "placeholder": "​",
      "style": "IPY_MODEL_717f3c33a56b40428ce424fe866813ad",
      "value": "Map: 100%"
     }
    },
    "e5ec1aa87c02490bb124209a6a0b9d58": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e897ee69fd504b358114a588cfd55adf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71122587d5da4615a3ae38c5f340d616",
      "placeholder": "​",
      "style": "IPY_MODEL_b7b450e0325e42f7b57ce38c54a899f8",
      "value": " 500/500 [00:00&lt;00:00, 1230.88 examples/s]"
     }
    },
    "e8e88fd9aaef43c5977a2df07b94bd07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c117cdc210234b74a6ee5a1698062f56",
       "IPY_MODEL_ec33104dd44b4519bb6dfba9f0b0ff29",
       "IPY_MODEL_e897ee69fd504b358114a588cfd55adf"
      ],
      "layout": "IPY_MODEL_b41171ae6d2e497ab994f317b15210fd"
     }
    },
    "ec33104dd44b4519bb6dfba9f0b0ff29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f151b4dec2f4479f866cd2066a431b50",
      "max": 500,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f1174aefcb804a22b95aed818c3b9f0f",
      "value": 500
     }
    },
    "f1174aefcb804a22b95aed818c3b9f0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f151b4dec2f4479f866cd2066a431b50": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
