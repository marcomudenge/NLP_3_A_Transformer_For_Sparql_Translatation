{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG1IIiGzZSXy"
      },
      "source": [
        "## <center> Interpreting SPARQL queries into understandable english questions using natural language processing <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doa6OMyLNtUG"
      },
      "source": [
        "### By:\n",
        "\n",
        "- Marco Mudenge\n",
        "- Andy Chen\n",
        "- Grover-Brando Tovar Oblitas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJTHc5UnZSX0"
      },
      "source": [
        "## Description\n",
        "\n",
        "In this notebook, we will build an automatic translator using the Transformer architecture. The idea is to use an automatic translation system to translate queries in SPARQL language into questions in English.\n",
        "\n",
        "#### What's SPARQL?\n",
        "SPARQL is a knowledge base query language, similar to SQL. Knowledge bases are a source of structured data, according to the standards, models and languages of the Semantic Web, which allow efficient access to a large quantity of information in a wide variety of fields. However, their access is limited by the complexity of the requests which does not allow the public to use them directly. It is also difficult for the uninformed user to understand the meaning of a request. We therefore want to code a Transformer type model which allows us to interpret a SPARQL query on the DBpedia knowledge base by associating it with a question in English.\n",
        "\n",
        "Thus, our machine translation system will take a SPARQL query as input and produce as output an English sentence corresponding to the question posed by the query. For example :\n",
        "\n",
        "__Enter__ _select distinct count ( ?uri ) where { dbr:Apocalypto dbo:language ?x . ?x dbp:region ?uri }_\n",
        "\n",
        "__Expected output__: _In how many other dbp:region do people live, whose dbo:language are spoken in dbr:Apocalypto?_\n",
        "\n",
        "You may have noticed that we reuse elements with the prefix dbr/dbo/dbp which are associated with data in DBpedia and the knowledge base schema. dbr:Apocalypto is simply a URI that describes a resource (or data) in DBpedia. Here is the URI in question: https://dbpedia.org/describe/?url=http%3A%2F%2Fdbpedia.org%2Fresource%2FApocalypto&sid=35407\n",
        "\n",
        "In this notebook, we will reproduce the Transformer architecture using Keras layers. We draw inspiration from the implementation of certain methods from the [Tensorflow](https://www.tensorflow.org/text/tutorials/transformer) tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHCvEjauZSX2"
      },
      "source": [
        "## DESCRIPTION OF DATA AND EVALUATION METRICS\n",
        "\n",
        "The corpus is a corpus of 5,000 pairs of questions - queries on DBPedia relating to a wide variety of more or less specific themes. Three data sets are provided:\n",
        "\n",
        "- The 4000 pairs of questions – training queries in a `train.csv` file.\n",
        "- The 500 pairs of questions – validation requests in a `validation.csv` file.\n",
        "- The 500 pairs of questions - test queries in a `test.csv` file\n",
        "\n",
        "The BLEU metric will be used to compare model translations to reference queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIZfzAD_ZSX2"
      },
      "source": [
        "## LET'S BEGIN!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-pZnJBUiWKj",
        "outputId": "274b7d79-bb4e-4c4f-830e-265c809aac7d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_text\n",
            "  Downloading tensorflow_text-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_text) (0.15.0)\n",
            "Requirement already satisfied: tensorflow<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_text) (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (1.59.0)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.15,>=2.14.0->tensorflow_text) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (3.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (3.2.2)\n",
            "Installing collected packages: tensorflow_text\n",
            "Successfully installed tensorflow_text-2.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AHulKdPZZSX2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_text\n",
        "import pathlib\n",
        "import re\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-46mweTdajyw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "root = os.getcwd() + \"/\" # To change if needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvMsQC9IZSX3"
      },
      "source": [
        "### 1 Data preparation\n",
        "\n",
        "We must first prepare the data before sending it to the translation system. For this, two classes will be used:\n",
        "- The `DataLoader` class will simply be used to read the data from the training and validation files\n",
        "- The `Preprocessor` class will be used to pre-process the data in an expected format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Yk674QnrZSX4"
      },
      "outputs": [],
      "source": [
        "class DataLoader:\n",
        "    \"\"\"\n",
        "     Class used to load data into DataFrame\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, training_path: str, validation_path: str) -> None:\n",
        "\n",
        "        self.train = pd.read_csv(training_path, sep=',', header=0).drop(columns=['id'])\n",
        "        self.val = pd.read_csv(validation_path, sep=',', header=0).drop(columns=['id'])\n",
        "\n",
        "    def get_train(self) -> pd.DataFrame:\n",
        "        return self.train\n",
        "\n",
        "    def get_val(self) -> pd.DataFrame:\n",
        "        return self.val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_9rzFQJNtUJ"
      },
      "source": [
        "#### 1.1 Pre-processing\n",
        "\n",
        "The `Preprocessor` class will perform the following transformations on SPARQL queries:\n",
        "- Replace all keywords (prefixes) of the form `dbx:` with `dbx_` (for example, `dbr:` becomes `dbr_` and `dbo:` becomes `dbo_`). Keywords that should be considered are: `dbr`, `dbo`, `dbp` and `rdf`\n",
        "- Replace all the following punctuation marks with words:\n",
        "   - `?` will become `var_`\n",
        "   - `{` will become `brack_open`\n",
        "   - `}` will become `brack_close`\n",
        "   - `(` will become `parent_open`\n",
        "   - `)` will become `parent_close`\n",
        "   - `.` will become `sep_dot`\n",
        "\n",
        "Regarding English questions, the `Preprocessor` class will perform the following transformations:\n",
        "- Remove `?` at the end of sentences\n",
        "- Replace all keywords of the form `dbx:` with `dbx_` (for example, `dbr:` becomes `dbr_` and `dbo:` becomes `dbo_`). Keywords that should be considered are: `dbr`, `dbo`, `dbp` and `rdf`\n",
        "- Will remove all unnecessary spaces before the start and after the end of the question\n",
        "\n",
        "This class also takes care of canceling the pre-processing once the Transformer has generated a sequence, which includes canceling the transformations indicated above and removing the start and end tokens of sentences which will have been added by the segmenter a little further down."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "20IAnMV4ZSX4"
      },
      "outputs": [],
      "source": [
        "class Preprocessor:\n",
        "    \"\"\"\n",
        "    Transforms and cleans data to improve model performance\n",
        "    \"\"\"\n",
        "\n",
        "    SPARQL_TRANSLATE_OBJECTS = {\n",
        "        \"dbr:\": \"dbr_\",\n",
        "        \"dbo:\": \"dbo_\",\n",
        "        \"dbp:\": \"dbp_\",\n",
        "        \"rdf:\": \"rdf_\"\n",
        "    }\n",
        "\n",
        "    SPARQL_TRANSLATE_SYMBOLS = {\n",
        "        \"?\": \"var_\",\n",
        "        \"{\": \"brack_open\",\n",
        "        \"}\": \"brack_close\",\n",
        "        \"(\": \"parent_open\",\n",
        "        \")\": \"parent_close\",\n",
        "        \".\": \"sep_dot\"\n",
        "    }\n",
        "\n",
        "    def transform_dataframe(self, data: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Transforms data from a DataFrame containing 'english' columns\n",
        "        and 'sparql'. Calls the functions `transform_sparql` and\n",
        "        `transform_english` on the correct columns\n",
        "\n",
        "        Args:\n",
        "            - data: Data to transform\n",
        "\n",
        "        Returns:\n",
        "            Transformed data\n",
        "        \"\"\"\n",
        "\n",
        "        data['sparql'] = data['sparql'].apply(self.transform_sparql)\n",
        "\n",
        "        data['english'] = data['english'].apply(self.transform_english)\n",
        "\n",
        "        return data\n",
        "\n",
        "    def transform_sparql(self, sparql: str):\n",
        "        \"\"\"\n",
        "        Transform a sparql query by replacing the \"dbx:\" tokens with \"dbx_\"\n",
        "        and replacing the punctuation marks with their equivalent in words\n",
        "        as indicated above\n",
        "\n",
        "        Args:\n",
        "            sparql: sparql query\n",
        "\n",
        "        Returns:\n",
        "            Sparql query transformed with the modifications mentioned above\n",
        "        \"\"\"\n",
        "\n",
        "        for key, value in self.SPARQL_TRANSLATE_OBJECTS.items():\n",
        "            sparql = sparql.replace(key, value)\n",
        "\n",
        "        for key, value in self.SPARQL_TRANSLATE_SYMBOLS.items():\n",
        "            sparql = sparql.replace(key, value)\n",
        "\n",
        "        return sparql\n",
        "\n",
        "    def transform_english(self, english: str):\n",
        "        \"\"\"\n",
        "        Transform a sparql query by replacing the \"dbx:\" tokens\n",
        "        with \"dbx_\" and removing the question marks as well as\n",
        "        unnecessary spaces at the beginning and end of the sentence\n",
        "\n",
        "        Args:\n",
        "            - english: English sentence to apply\n",
        "            the transformations\n",
        "\n",
        "        Returns:\n",
        "            Sentence transformed with the modifications mentioned above\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        for key, value in self.SPARQL_TRANSLATE_OBJECTS.items():\n",
        "            english = english.replace(key, value)\n",
        "\n",
        "        english = english.replace('?', '')\n",
        "\n",
        "        english = english.strip()\n",
        "\n",
        "        return english\n",
        "\n",
        "    def transform_back_english(self, english):\n",
        "        \"\"\"\n",
        "        Performs reverse transformations of the English sentence\n",
        "        (replaces dbx_ in dbx:).\n",
        "        Be careful, this function must also remove the start tokens\n",
        "        and end of a sentence which are added when\n",
        "        segmentation (tokenization)\n",
        "\n",
        "        Args:\n",
        "            - english: Sentence generated by a model containing the tokens\n",
        "            start and end\n",
        "\n",
        "        Returns:\n",
        "            - English sentence whose transformations have been undone\n",
        "        \"\"\"\n",
        "        english = bytes(tf.squeeze(english).numpy()).decode()\n",
        "\n",
        "        english = english.replace(' _ ', '_')\n",
        "\n",
        "        for key, value in self.SPARQL_TRANSLATE_OBJECTS.items():\n",
        "            english = english.replace(value, key)\n",
        "\n",
        "        return english\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLoWLllQNtUJ"
      },
      "source": [
        "Testing the `Preprocessor` class below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9gWqQBMQNtUJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a6b9cac-c855-407d-f864-0d28a23532d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformed sparql : \n",
            "select distinct count parent_open var_uri parent_close where brack_open var_uri dbo_director dbr_Stanley_Kubrick sep_dot brack_close\n",
            "select distinct var_uri where brack_open var_uri dbo_founder dbr_John_Forbes_parent_openBritish_Army_officerparent_close sep_dot var_uri rdf_type dbo_City brack_close\n",
            "\n",
            "Transformed english : \n",
            "how many movies are there whose dbo_director is dbr_Stanley_Kubrick\n",
            "what dbo_City's dbo_founder is dbr_John_Forbes_(British_Army_officer)\n"
          ]
        }
      ],
      "source": [
        "def test_preprocessor():\n",
        "\n",
        "    test_queries = [\n",
        "        'select distinct count ( ?uri ) where { ?uri dbo:director dbr:Stanley_Kubrick . }',\n",
        "        'select distinct ?uri where { ?uri dbo:founder dbr:John_Forbes_(British_Army_officer) . ?uri rdf:type dbo:City }'\n",
        "    ]\n",
        "\n",
        "    test_english = [\n",
        "        'how many movies are there whose dbo:director is dbr:Stanley_Kubrick ?',\n",
        "        'what dbo:City\\'s dbo:founder is dbr:John_Forbes_(British_Army_officer) ?'\n",
        "    ]\n",
        "\n",
        "    preprocessor = Preprocessor()\n",
        "    print('Transformed sparql : ')\n",
        "    for query in test_queries:\n",
        "        print(preprocessor.transform_sparql(query))\n",
        "\n",
        "    print()\n",
        "    print('Transformed english : ')\n",
        "    for english in test_english:\n",
        "        print(preprocessor.transform_english(english))\n",
        "\n",
        "\n",
        "test_preprocessor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8aZqWzUNtUK"
      },
      "source": [
        "Expected output :\n",
        "\n",
        "```\n",
        "Transformed sparql :\n",
        "select distinct count parent_open var_uri parent_close where brack_open var_uri dbo_director dbr_Stanley_Kubrick sep_dot brack_close\n",
        "\n",
        "select distinct var_uri where brack_open var_uri dbo_founder dbr_John_Forbes_parent_openBritish_Army_officerparent_close sep_dot var_uri rdf_type dbo_City brack_close\n",
        "\n",
        "Transformed english :\n",
        "how many movies are there whose dbo_director is dbr_Stanley_Kubrick\n",
        "\n",
        "what dbo_City's dbo_founder is dbr_John_Forbes_(British_Army_officer)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0kmklbnNtUK"
      },
      "source": [
        "We can now instantiate an object from the\n",
        "`Data Loader` class to load training and validation data from `train.csv` and `validation.csv` files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3L0LoK9-ZSX4"
      },
      "outputs": [],
      "source": [
        "data_loader = DataLoader(\n",
        "    training_path=root + 'train.csv',\n",
        "    validation_path=root +'validation.csv'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLR3RMbFNtUK"
      },
      "source": [
        "Apply data pre-processing on previously loaded data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "d1eNYoOTZSX4"
      },
      "outputs": [],
      "source": [
        "pre_processor = Preprocessor()\n",
        "processed_train = pre_processor.transform_dataframe(data_loader.train)\n",
        "processed_val = pre_processor.transform_dataframe(data_loader.val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab9aHTYkZSX5"
      },
      "source": [
        "### 2. Segmentation (tokenization)\n",
        "\n",
        "Once the data is imported and modified, the sentences must be adapted into a format that the model can understand.\n",
        "\n",
        "First of all, we need to segment the sentences into tokens. For this, a dictionary of words (vocabulary) will be necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBCz_Eu-NtUL"
      },
      "source": [
        "#### 2.0 LanguageTokenizer\n",
        "\n",
        "The `LanguageTokenizer` class will take care of creating this vocabulary and transforming sentences of a specific language into tokens. In our case, there will be 2 instances of this class: one for English and the other for sparql. This class has several functions that will be very useful to us including `create_vocab` to create the vocabulary of the model, `tokenize` to transform sentences into tokens and `detokenize` to transform tokens into sentences.\n",
        "\n",
        "We will use Bert's segmenter to find the tokens and vocabulary. The segmenter parameters are given to you. This segmenter divides each word into word parts. For example \"characteristically\" will be segmented into 'characteristic' and '##ally'.\n",
        "\n",
        "Then, for each of the sentences, after transforming them into tokens, you will need to add the start (`[START]`) and end of sentence (`[END]`) tokens. This operation will be carried out in the `add_start_end` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aQsquspb7esn"
      },
      "outputs": [],
      "source": [
        "class LanguageTokenizer(tf.Module):\n",
        "    \"\"\"\n",
        "    Class representing a tokenizer for a specific language.\n",
        "    In our case, there will be one for sparql and one for English\n",
        "    \"\"\"\n",
        "\n",
        "    reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "    START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
        "    END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
        "\n",
        "    tokenizer_params = dict(lower_case=True)\n",
        "\n",
        "    vocab_args = dict(\n",
        "        vocab_size = 8000,\n",
        "        reserved_tokens=reserved_tokens,\n",
        "        bert_tokenizer_params=tokenizer_params,\n",
        "        learn_params=None,\n",
        "    )\n",
        "\n",
        "    def __init__(self, reserved_tokens, vocab_path):\n",
        "        \"\"\"\n",
        "        Initializes the BertTokenizer using the `vocab_path` parameter\n",
        "        and putting the tokenizer in “lower case” mode.\n",
        "\n",
        "        Args:\n",
        "            - reserved_tokens: Reserved tokens from the BertTokenizer\n",
        "            - vocab_path: Path to the file containing the tokenizer vocabulary\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__(name=\"LanguageTokenizer\")\n",
        "\n",
        "        # Retrieve the vocabulary save as a file needed for tokenization\n",
        "        with open(vocab_path) as f:\n",
        "            f = open(vocab_path, 'r')\n",
        "\n",
        "            init = tf.lookup.TextFileInitializer(\n",
        "                    f.name,\n",
        "                    key_dtype=tf.string, key_index=tf.lookup.TextFileIndex.WHOLE_LINE,\n",
        "                    value_dtype=tf.int64, value_index=tf.lookup.TextFileIndex.LINE_NUMBER)\n",
        "\n",
        "            lookup_table = tf.lookup.StaticVocabularyTable(\n",
        "                init,\n",
        "                num_oov_buckets=1\n",
        "            )\n",
        "\n",
        "            f.close()\n",
        "\n",
        "        self.tokenizer = tensorflow_text.BertTokenizer(lookup_table, **self.tokenizer_params)\n",
        "\n",
        "        self.reserved_tokens = reserved_tokens\n",
        "\n",
        "    def create_vocab(language_sentences: pd.DataFrame, path: str):\n",
        "        \"\"\"\n",
        "        Creates a vocabulary from the input sentences\n",
        "        (language_sentences). For this we use the bert_vocab_from_dataset()\n",
        "        function.\n",
        "\n",
        "        Once the vocabulary has been created, it will need to be saved in a specified file\n",
        "        by the `path` attribute.\n",
        "\n",
        "        Args:\n",
        "            - language_sentences: DataFrame containing language sentences\n",
        "            - path: Path where the vocabulary will be saved\n",
        "        \"\"\"\n",
        "\n",
        "        # Convert the sentence DataFrame into a Dataset\n",
        "        vocab_tf_dataset = tf.data.Dataset.from_tensor_slices((language_sentences))\n",
        "\n",
        "        # Create the vocabulary\n",
        "        vocab = bert_vocab.bert_vocab_from_dataset(vocab_tf_dataset, **LanguageTokenizer.vocab_args)\n",
        "\n",
        "        # Save the vocabulary to the specified file\n",
        "        with open(path, 'w', encoding='utf-8') as f:\n",
        "            f.write('\\n'.join(vocab))\n",
        "            f.close()\n",
        "\n",
        "    @tf.function\n",
        "    def tokenize(self, inputs):\n",
        "        \"\"\"\n",
        "        Transforms sentences into token indexes and adds the\n",
        "        start and end tokens.\n",
        "\n",
        "        Args:\n",
        "            - inputs: Input sentences\n",
        "\n",
        "        Returns:\n",
        "            Tokens matching the sentence with start and end tokens\n",
        "        \"\"\"\n",
        "\n",
        "        # Tokenize the inputs into a ragged tensor\n",
        "        tokens = self.tokenizer.tokenize(inputs).merge_dims(1,2)\n",
        "\n",
        "        # Add the start and end token to the tokenized inputs\n",
        "        tokens = LanguageTokenizer.add_start_end(tokens)\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    @tf.function\n",
        "    def detokenize(self, tokenized):\n",
        "        \"\"\"\n",
        "        Transforms a list of indexes into tokens. Then apply\n",
        "        the `cleanup_text` method to clean\n",
        "        the data.\n",
        "\n",
        "        Args:\n",
        "            - tokenized: List of tokens\n",
        "\n",
        "        Returns:\n",
        "            Sentence corresponding to tokens\n",
        "        \"\"\"\n",
        "\n",
        "        # Detokenize: turn the token IDs back into text returned as a ragged tensor\n",
        "        detokenized = self.tokenizer.detokenize(tokenized)\n",
        "\n",
        "        # Cleaning the detokenized text tensor back into a regular string\n",
        "        detokenized = LanguageTokenizer.cleanup_text(self.reserved_tokens, detokenized)\n",
        "\n",
        "        return detokenized\n",
        "\n",
        "    def add_start_end(tokenized_sentences):\n",
        "        \"\"\"\n",
        "        Function that adds the representation of the [START] and [END] tokens to the input sentence\n",
        "\n",
        "        Args:\n",
        "            - tokenized_sentences: Tensor containing the token indices of the sentences\n",
        "\n",
        "        Returns:\n",
        "            Initial tensor with token indices [START] and [END] at start and end\n",
        "        \"\"\"\n",
        "\n",
        "        # Create the start and end tokens tensors. Dimensions should match the number of sentences\n",
        "        start = tf.fill([tokenized_sentences.bounding_shape()[0], 1], LanguageTokenizer.START)\n",
        "        end = tf.fill([tokenized_sentences.bounding_shape()[0], 1], LanguageTokenizer.END)\n",
        "\n",
        "        # Make the data type compatible with the tokenized_sentences tensor\n",
        "        start = tf.cast(start, tf.int64)\n",
        "        end = tf.cast(end, tf.int64)\n",
        "\n",
        "        # Concatenate the start tokens, sentences and end tokens\n",
        "        tokenized_sentences = tf.concat([start, tokenized_sentences, end], axis=1)\n",
        "\n",
        "        return tokenized_sentences\n",
        "\n",
        "\n",
        "    def cleanup_text(reserved_tokens, token_txt):\n",
        "        \"\"\"\n",
        "        Function that cleans up text generated by the BertTokenizer detokenize() function.\n",
        "        Args:\n",
        "            - reserved_tokens: Reserved tokens from the BertTokenizer\n",
        "            - token_text: String generated by the detokenize() function\n",
        "\n",
        "        Returns:\n",
        "            Cleaned text\n",
        "        \"\"\"\n",
        "\n",
        "        # Determine which tokens are reserved based on the reserved_tokens\n",
        "        bad_tokens = [re.escape(token) for token in reserved_tokens if token != \"[UNK]\"] # Tokens to remove are all non-[UNK] tokens\n",
        "\n",
        "        # Create a regular expression pattern from the bad_tokens\n",
        "        bad_token_regex = tf.strings.join(bad_tokens, \"|\")\n",
        "\n",
        "        # Determine which tokens match the bad_token_regex\n",
        "        bad_cells = tf.strings.regex_full_match(token_txt, bad_token_regex)\n",
        "\n",
        "        # Replace the bad tokens with an empty string\n",
        "        cleaned_cells = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
        "\n",
        "        # Joining the good cells back into text\n",
        "        cleaned_cells = tf.strings.reduce_join(cleaned_cells, separator=' ', axis=-1)\n",
        "\n",
        "        return cleaned_cells"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the `LanguageTokenizer` class below"
      ],
      "metadata": {
        "id": "xI0SrT_UsM35"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LmMtHCK7NtUL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29ed46f5-e177-49af-d701-0df361b3742a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 320, 24, ..., 23, 21, 3], [2, 43, 45, 102, 30, 3]]\n"
          ]
        }
      ],
      "source": [
        "def test_add_start_end():\n",
        "\n",
        "    tokenized_sentence = tf.ragged.constant([[320, 24, 500, 23, 21], [43, 45, 102, 30]], dtype=tf.int64)\n",
        "    tf.print(LanguageTokenizer.add_start_end(tokenized_sentence))\n",
        "\n",
        "test_add_start_end()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohhyoPrlNtUM"
      },
      "source": [
        "Expected output:\n",
        "```\n",
        "[[2, 320, 24, ..., 23, 21, 3], [2, 43, 45, 102, 30, 3]]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bpF5kkogNtUM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e327c13-efdc-41b7-beba-4b85af2a880c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary :  [PAD] [UNK] [START] [END] . ? a b d e h i k m n o p r s t u w y ##. ##? ##a ##b ##d ##e ##h ##i ##k ##m ##n ##o ##p ##r ##s ##t ##u ##w ##y\n",
            "Tokenized sentence : <tf.RaggedTensor [[2, 10, 34, 40, 13, 25, 33, 41, 20, 4, 18, 16, 36, 28, 37, 30, 27, 28,\n",
            "  33, 38, 37, 21, 28, 36, 28, 7, 34, 36, 33, 11, 33, 14, 28, 40, 22, 34,\n",
            "  36, 31, 5, 3]]>\n",
            "Detokenized sentence : how many u . s presidents were born in new york ?\n"
          ]
        }
      ],
      "source": [
        "def test_tokenizer():\n",
        "\n",
        "    sentence = ['how many U.S Presidents were born in New York ?']\n",
        "    vocab_path = root + 'test_language_vocab.txt'\n",
        "    LanguageTokenizer.create_vocab(sentence, vocab_path)\n",
        "\n",
        "    with open(vocab_path) as f:\n",
        "        vocab = f.read()\n",
        "\n",
        "    print('Vocabulary : ', vocab.replace('\\n', ' '))\n",
        "    test_tokenizer_obj = LanguageTokenizer(LanguageTokenizer.reserved_tokens, vocab_path)\n",
        "    tokenized_sentence = test_tokenizer_obj.tokenize(sentence)\n",
        "    tf.print(f'Tokenized sentence : {tokenized_sentence}')\n",
        "\n",
        "    detokenized_sentence = test_tokenizer_obj.detokenize(tokenized_sentence)\n",
        "    tf.print(f'Detokenized sentence : {bytes(tf.squeeze(detokenized_sentence).numpy()).decode()}') # TODO: Would fail for batched inputs\n",
        "\n",
        "test_tokenizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Mqz0qt_NtUM"
      },
      "source": [
        "Expected output:\n",
        "```\n",
        "Vocabulary :  [PAD] [UNK] [START] [END] . ? a b d e h i k m n o p r s t u w y ##. ##? ##a ##b ##d ##e ##h ##i ##k ##m ##n ##o ##p ##r ##s ##t ##u ##w ##y\n",
        "Tokenized sentence : <tf.RaggedTensor [[2, 10, 34, 40, 13, 25, 33, 41, 20, 4, 18, 16, 36, 28, 37, 30, 27, 28,\n",
        "  33, 38, 37, 21, 28, 36, 28, 7, 34, 36, 33, 11, 33, 14, 28, 40, 22, 34,\n",
        "  36, 31, 5, 3]]>\n",
        "Detokenized sentence : how many u . s presidents were born in new york ?\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUzFpotvNtUM"
      },
      "source": [
        "#### 2.1 Vocabulary\n",
        "\n",
        "We can now create the vocabulary for each language using the `create_vocab` function. We store English vocabulary in a file called `language_vocab_english.txt` and sparql vocabulary in a file called `language_vocab_sparql.txt`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "55klqAgxCH_Q"
      },
      "outputs": [],
      "source": [
        "sparql_vocab_path = root + 'language_vocab_sparql.txt'\n",
        "english_vocab_path = root + 'language_vocab_english.txt'\n",
        "\n",
        "# Create the vocabulary for the sparql and english sentences\n",
        "#LanguageTokenizer.create_vocab(processed_train['sparql'], sparql_vocab_path)\n",
        "#LanguageTokenizer.create_vocab(processed_train['english'], english_vocab_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBNn1Wp3NtUM"
      },
      "source": [
        "In order to only use one class, we will create a class that groups the two tokenizers into a single class called GroupedTokenizers. Complete the constructor which initializes the english attribute corresponding to the english tokenizer and the sparql attribute corresponding to the sparql tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eza5AHd_C054"
      },
      "outputs": [],
      "source": [
        "class GroupedTokenizers(tf.Module):\n",
        "    \"\"\"\n",
        "    This class brings together the two segmenters (tokenizers) which will be\n",
        "    used (one for each language)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, reserved_tokens, vocab_english_path: str, vocab_sparql_path: str):\n",
        "        \"\"\"\n",
        "        Initializes the two tokenizers (english and sparql)\n",
        "        Args:\n",
        "            - reserved_tokens: Reserved tokens from the BertTokenizer\n",
        "            - vocab_english_path: Path to the file containing\n",
        "            the English vocabulary of the tokenizer\n",
        "            - vocab_sparql_path: Path to the file containing\n",
        "            the sparql vocabulary of the segmenter (tokenizer)\n",
        "        \"\"\"\n",
        "        self.english = LanguageTokenizer(reserved_tokens, vocab_english_path)\n",
        "        self.sparql = LanguageTokenizer(reserved_tokens, vocab_sparql_path)\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNUhLzCNNtUN"
      },
      "source": [
        "The following test verifies that the pre-processing and tokenizer are working correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BVLoJCRLDhVX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "617c94e6-d686-4f4a-9a5b-36b380324be7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English : \n",
            " how many movies are there whose dbo:director is dbr:Stanley_Kubrick ? \n",
            "\n",
            "Processed english : \n",
            " how many movies are there whose dbo_director is dbr_Stanley_Kubrick \n",
            "\n",
            "Tokenized english : \n",
            " <tf.RaggedTensor [[2, 74, 75, 495, 67, 73, 65, 61, 25, 228, 59, 60, 25, 896, 95, 261, 25,\n",
            "  36, 116, 329, 757, 114, 3]]> \n",
            "\n",
            "Detokenized english : \n",
            " 0    how many movies are there whose dbo:director i...\n",
            "dtype: object \n",
            "\n",
            "\n",
            "------------------------------------------------\n",
            "\n",
            "Sparql : \n",
            " select distinct count ( ?uri ) where { ?uri dbo:director dbr:Stanley_Kubrick . } \n",
            "\n",
            "Processed sparql : \n",
            " select distinct count parent_open var_uri parent_close where brack_open var_uri dbo_director dbr_Stanley_Kubrick sep_dot brack_close \n",
            "\n",
            "Tokenized sparql : \n",
            " <tf.RaggedTensor [[2, 66, 65, 71, 68, 22, 63, 55, 22, 56, 68, 22, 62, 64, 57, 22, 63, 55,\n",
            "  22, 56, 61, 22, 208, 59, 22, 41, 182, 80, 233, 22, 33, 879, 703, 111,\n",
            "  60, 22, 58, 57, 22, 62, 3]]> \n",
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizers = GroupedTokenizers(\n",
        "    LanguageTokenizer.reserved_tokens,\n",
        "    root + 'language_vocab_english.txt',\n",
        "    root + 'language_vocab_sparql.txt'\n",
        ")\n",
        "\n",
        "def test_tokenizer_preprocessor(tokenizers: GroupedTokenizers):\n",
        "    \"\"\"\n",
        "    Check that the tokenizer and preprocessor functions are correct\n",
        "    and well coded. If they are, the initial English sentences and\n",
        "    sparql should be the same as the input\n",
        "\n",
        "    \"\"\"\n",
        "    english = 'how many movies are there whose dbo:director is dbr:Stanley_Kubrick ?'\n",
        "    sparql = 'select distinct count ( ?uri ) where { ?uri dbo:director dbr:Stanley_Kubrick . }'\n",
        "    print('English : \\n', english, '\\n')\n",
        "\n",
        "    # processed_train = pre_processor.transform_dataframe\n",
        "    pre_processor = Preprocessor()\n",
        "    processed_english = pre_processor.transform_english(english)\n",
        "    processed_sparql = pre_processor.transform_sparql(sparql)\n",
        "\n",
        "    print('Processed english : \\n', processed_english, '\\n')\n",
        "    tokenized_english = tokenizers.english.tokenize(processed_english)\n",
        "    print('Tokenized english : \\n', tokenized_english, '\\n')\n",
        "    detokenized_english = pd.Series(tokenizers.english.detokenize(tokenized_english).numpy())\n",
        "    print('Detokenized english : \\n', detokenized_english.apply(pre_processor.transform_back_english), '\\n')\n",
        "    print()\n",
        "    print('------------------------------------------------')\n",
        "    print()\n",
        "\n",
        "    print('Sparql : \\n', sparql, '\\n')\n",
        "\n",
        "    print('Processed sparql : \\n', processed_sparql, '\\n')\n",
        "    tokenized_sparql = tokenizers.sparql.tokenize(processed_sparql)\n",
        "    print('Tokenized sparql : \\n', tokenized_sparql, '\\n')\n",
        "\n",
        "test_tokenizer_preprocessor(tokenizers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njcViPKN1ndG"
      },
      "source": [
        "### 3. Batching\n",
        "\n",
        "Given the large amount of data involved in training a model, it is important to send the data as efficiently as possible. To do this, the data is grouped into small groups called “batches”. This makes it possible to process several elements in parallel and considerably reduces training time.\n",
        "\n",
        "For this, the `Batcher` class will be used. This class takes care of grouping the data into small batches and preparing them to send to the model. This class has several functions:\n",
        "- `make_batches`: It receives as a parameter an instance of the `tf.Dataset` class. It then divides the dataset into small batches and sends them to the `prepare_batch` function\n",
        "- `prepare_batch`: Receives a batch and prepares it by performing the following transformations:\n",
        "   - Segments input sentences using the correct tokenizers passed as parameters in the constructor\n",
        "   - Ensures sentence size does not exceed `max_tokens`\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/marcomudenge/NLP_3_A_Transformer_For_Sparql_Translation/blob/main/Batcher.png?raw=1\" alt=\"Batcher\" width=\"100%\" height=\"700\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "y0jovaYiEdh8"
      },
      "outputs": [],
      "source": [
        "class Batcher():\n",
        "    \"\"\"\n",
        "    Cette classe s'occupe de regrouper les données en petits groupes (batches) et\n",
        "    de préparer les données pour les envoyer au modèle.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tokenizers: GroupedTokenizers, train, max_tokens, batch_size, buffer_size):\n",
        "        \"\"\"\n",
        "        Initialise les paramètres en entrée\n",
        "\n",
        "        Args :\n",
        "            - tokenizers : tokenizers pour transformer les entrées en jeton\n",
        "            - train : Valeur booléenne pour savoir si les batches seront utilisées\n",
        "            pour de l'entrainement ou pas\n",
        "            - max_tokens : Nombre de jetons maximums pour une entrée\n",
        "            - batch_size : Taille des groupes (batches)\n",
        "            - buffer_size : Taille du buffer servant à mélanger les données dans le\n",
        "            cas de l'entrainement\n",
        "        \"\"\"\n",
        "\n",
        "        self.tokenizers = tokenizers\n",
        "        self.train = train\n",
        "        self.max_tokens = max_tokens\n",
        "        self.buffer_size = buffer_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def prepare_batch(self, input_language, output_language=None):\n",
        "        \"\"\"\n",
        "        Prepares batches to send to the model. This function is\n",
        "        called for each element of a Tensorflow Dataset.\n",
        "\n",
        "        Performs the following transformations:\n",
        "            - Tokenize the input sentences using the correct tokenizers passed\n",
        "            as a parameter in the constructor\n",
        "            - Ensures sentence size does not exceed `max_tokens` (max_tokens\n",
        "            is included)\n",
        "\n",
        "        Args:\n",
        "            - input_language: Input in the input language (sparql in our case)\n",
        "            of size (self.batch_size, x)\n",
        "            - output_language: Output in the output language (english in our case)\n",
        "            of the size (self.batch_size, x). None in the case of test batches\n",
        "\n",
        "        Returns:\n",
        "            - If self.train == True:\n",
        "                Returns a tuple of the form ((input_language, output_language_inputs), output_language_labels)\n",
        "                which will be the respective inputs of the encoder and decoder and the\n",
        "                decoder output.\n",
        "\n",
        "                Here's what each return value represents\n",
        "                - input_language: tensor containing the tokens of the `input_language` parameter\n",
        "                limited to `max_tokens`\n",
        "                - output_language_inputs: tensor containing the parameter tokens\n",
        "                `output_language` limited to `max_tokens`+1 (to allow predicting the next\n",
        "                token)\n",
        "                - output_language_labels: tensor containing the parameter tokens\n",
        "                `output_language` containing the next character\n",
        "\n",
        "            - If self.train == False:\n",
        "                Returns a tuple of the form (input_language, output_language) which\n",
        "                represent the encoder input and a\n",
        "                output tensor initialized with input token size\n",
        "                (self.batch_size,). Return values are explained above\n",
        "        \"\"\"\n",
        "\n",
        "        # Tokenize the input sentences\n",
        "        input_language = self.tokenizers.sparql.tokenize(input_language)\n",
        "        input_language = input_language[:, :(self.max_tokens+1)].to_tensor()\n",
        "\n",
        "        # If we're in training mode, tokenize the output sentences\n",
        "        if (self.train == True):\n",
        "            # Tokenize the output sentences\n",
        "            output_language = self.tokenizers.english.tokenize(output_language)\n",
        "            output_language = output_language[:, :(self.max_tokens+1)].to_tensor()\n",
        "            output_language_inputs = output_language[:, :-1]\n",
        "            output_language_labels = output_language[:, 1:]\n",
        "\n",
        "            return (input_language, output_language_inputs), output_language_labels\n",
        "        else:\n",
        "            # If we're not in training mode, return the input tokens and output tensor initialized with the start token\n",
        "            #output_language = self.tokenizers.english.tokenize(tf.fill((self.batch_size,), ''))\n",
        "            output_language = self.tokenizers.english.tokenize(tf.fill((tf.shape(input_language)[0],), ''))\n",
        "            output_language = output_language[:, :(self.max_tokens+1)]\n",
        "            output_language_labels = output_language[:, :-1].to_tensor()\n",
        "            return input_language, output_language_labels\n",
        "\n",
        "    def make_batches(self, ds):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            - ds: Dataset containing the examples of the form\n",
        "            ((sparql, english_in), english_label)\n",
        "            if self.train == True and form (sparql, english)\n",
        "            if self.train == False\n",
        "\n",
        "        Returns:\n",
        "            The initial dataset (mixed if self.train == True) containing\n",
        "            elements of the size of self.batch_size including the self.prepare_batch function\n",
        "            was called on each of the elements and whose elements are\n",
        "            prefetched. If self.train == False, it's the same principle,\n",
        "            but the data is not mixed\n",
        "        \"\"\"\n",
        "\n",
        "        # Shuffle the dataset if it is for training\n",
        "        if (self.train == True):\n",
        "            ds = ds.shuffle(self.buffer_size)\n",
        "\n",
        "        # Prepare the batches\n",
        "        ds = ds.batch(self.batch_size)\n",
        "        ds = ds.map(self.prepare_batch)\n",
        "\n",
        "        return ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO7GdsW1NtUN"
      },
      "source": [
        "You can now test the batcher using the following function (check that the output of the decoder contains one more token than the sentence that enters the decoder and that what enters the encoder is indeed sparql )."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "vw_c2-CKNtUN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf455dcb-848f-4be2-b53b-7f34842e9530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2 66 65 ... 63 55 22]\n",
            " [2 66 65 ... 64 57 22]]\n",
            "[[2]\n",
            " [2]]\n"
          ]
        }
      ],
      "source": [
        "def test_batcher(tokenizers):\n",
        "\n",
        "\n",
        "    english = pd.Series([\n",
        "        'how many movies are there whose dbo_director is dbr_Stanley_Kubrick',\n",
        "        'what is the dbo_River whose dbo_riverMouth is dbr_Dead_Sea',\n",
        "    ])\n",
        "\n",
        "    sparql = pd.Series([\n",
        "        'select distinct count parent_open var_uri parent_close where brack_open var_uri dbo_director dbr_Stanley_Kubrick sep_dot brack_close',\n",
        "        'select distinct var_uri where brack_open var_uri dbo_riverMouth dbr_Dead_Sea sep_dot var_uri rdf_type dbo_River brack_close',\n",
        "    ])\n",
        "\n",
        "    train = False\n",
        "    batcher = Batcher(tokenizers, train, 8, 64, 20000)\n",
        "\n",
        "    val_english = tf.data.Dataset.from_tensor_slices(english)\n",
        "    val_sparql = tf.data.Dataset.from_tensor_slices(sparql)\n",
        "    val_examples = tf.data.Dataset.zip((val_sparql, val_english))\n",
        "\n",
        "    batches = batcher.make_batches(val_examples)\n",
        "    for x in batches:\n",
        "      if train:\n",
        "        tf.print('Detokenized inputs encoder : ', tokenizers.sparql.detokenize(x[0][0]))\n",
        "        tf.print('Detokenized inputs decoder  : ', tokenizers.english.detokenize(x[0][1]))\n",
        "        tf.print('Detokenized outputs decoder : ', tokenizers.english.detokenize(x[1]))\n",
        "\n",
        "        concat = tf.concat([x[0][0], x[0][1], x[1]], axis=1)\n",
        "        print('Concatened values : ', concat)\n",
        "      else:\n",
        "        tf.print(x[0])\n",
        "        tf.print(x[1])\n",
        "\n",
        "test_batcher(tokenizers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Uy4egm3N_py"
      },
      "source": [
        "### 4. Transformer\n",
        "\n",
        "<img style=\"float: right;\" src=\"https://github.com/marcomudenge/NLP_3_A_Transformer_For_Sparql_Translation/blob/main/Transformer.png?raw=1\" alt=\"Transformer\" width=\"500\" height=\"700\"/>\n",
        "\n",
        "Now that the data is ready to be sent to the model, all that remains is to create its architecture. For this, the Keras library will be used. Keras is a library that is built on top of Tensorflow to facilitate the development of models in an object-oriented style. Since Tensorflow 2.0, it is now directly integrated into Tensorflow. For more details, the documentation is present on this [site](https://keras.io/api/)\n",
        "\n",
        "The architecture that will be followed in this notebook is presented in the image on the right. The list of layers that will be implemented are as follows:\n",
        "- `Positional Embedding`: Allows the generation of position embeddings\n",
        "- `Global-Self Attention`: Takes care of the encoder's attention mechanism\n",
        "- `Feed Forward`: Allows you to connect inputs and outputs with a neural network\n",
        "- `Decoder Attention`: Takes care of the first attention mechanism of the decoder\n",
        "- `Cross Attention`: Takes care of the second attention mechanism of the decoder (connects the encoder to the decoder)\n",
        "\n",
        "The addition and normalization layers will be included in the previous layers. For example, the `Add & Norm` layer that follows the `Global-Self Attention` layer in the graph will be included in the `Global-Self Attention` layer.\n",
        "\n",
        "Then, layers will also be used to group these layers together to simplify the Transformer pipeline. Here is the list of layers that will be added to those on the graph:\n",
        "- `Encoder Layer`: Represents a single encoder containing the `Global-Self Attention` and `Feed Forward` layers\n",
        "- `Decoder Layer`: Represents a single decoder containing the `Decoder Attention`, `Cross Attention` and `Feed Forward` layers\n",
        "- `Encoder`: Represents several encoders in parallel\n",
        "- `Decoder`: Represents several decoders in parallel\n",
        "- `Transformer`: Represents the entire Transformer and includes all encoders, decoders and embedding layers\n",
        "\n",
        "Each layer will be created manually and implemented as a Keras layer. If you're not familiar with Keras, here are some tutorials that might help:\n",
        "- https://keras.io/api/models/model/\n",
        "- https://www.tensorflow.org/text/tutorials/transformer\n",
        "- https://machinelearningmastery.com/implementing-the-transformer-encoder-from-scratch-in-tensorflow-and-keras/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z4VEorxV_DL"
      },
      "source": [
        "#### 4.1 Positional Embedding\n",
        "\n",
        "To allow the model to take into account the order of the tokens passed to it, it is important to pass information to the model about the position of the tokens in a sentence. The `PositionalEmbedding` layer takes care of this. Using the following formula, position embeddings are generated, allowing a token's position to be incorporated into its embedding:\n",
        "$$PE_{(pos, 2i)} = sin \\Big( \\frac{pos}{10000^{2i/d_{model}}} \\Big)$$\n",
        "$$PE_{(pos, 2i+1)} = cos \\Big( \\frac{pos}{10000^{2i/d_{model}}} \\Big)$$\n",
        "\n",
        "where $d_{model}$ is the dimension of the output embeddings and $i$ is simply the index of a value in the embedding vector.\n",
        "\n",
        "The `generate_positional_embedding` function generates positional embeddings. This takes as input:\n",
        "- `length`: Maximum number of tokens for which the position embedding must be generated\n",
        "- `depth`: Dimension of the model embeddings.\n",
        "\n",
        "The `call` function of this layer is called with the following parameter (tensor sizes are indicated in parentheses):\n",
        "- `x` (of size [batch_size, input_size] where the batch_size is the number of elements that are sent at a time for one iteration of the training and input_size is the maximum size of the input sentences): Layer inputs . This corresponds in particular to the tensor containing the indices of each token corresponding to the sentence\n",
        "  \n",
        "\n",
        "It returns the embedding of the input in the latent space including the positions of the tokens (batch_size, input_size, dim_model).\n",
        "\n",
        "The `call` function must perform the following operations:\n",
        "1. Call the `embedding_layer` layer which generates embeddings relative to the inputs\n",
        "2. Multiply each value by the root of `dim_model` (This multiplication is used to enlarge the embeddings so that they are of an order of magnitude comparable to the position embeddings that are added subsequently. For more details, see original article leading to the creation of the Transformer entitled \"Attention Is All You Need\").\n",
        "3. Then add the position embeddings to the embeddings generated by the `embedding_layer` (after they have been multiplied by the root of `dim_model`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "czAWrftUdkvq"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Class representing the step which incorporates the positions of the tokens into the latent space\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, dim_model):\n",
        "        \"\"\"\n",
        "        Initializes a layer of embeddings and position embeddings\n",
        "\n",
        "        Args:\n",
        "            - input_size: Input size of the layer (vocabulary size)\n",
        "            - dim_model: Size of model embeddings (size of layer output embedding)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.embedding_layer = tf.keras.layers.Embedding(input_size, dim_model, mask_zero=True)\n",
        "        self.position_embeddings = self.generate_positions_embedding(length=2048, depth=dim_model)\n",
        "        self.dim_model = dim_model\n",
        "\n",
        "    def compute_mask(self, *args, **kwargs):\n",
        "        return self.embedding_layer.compute_mask(*args, **kwargs)\n",
        "\n",
        "    def generate_positions_embedding(self, length, depth):\n",
        "        depth = depth/2\n",
        "\n",
        "        positions = np.arange(length)[:, np.newaxis]\n",
        "        depths = np.arange(depth)[np.newaxis, :]/depth\n",
        "\n",
        "        angle_rates = 1 / (10000**depths)\n",
        "        angle_rads = positions * angle_rates\n",
        "\n",
        "        pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n",
        "\n",
        "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "    def call(self, x):\n",
        "        \"\"\"\n",
        "        Runs layer embeddings on the input normalizing it to the root of the output dimension\n",
        "        \"\"\"\n",
        "        x = self.embedding_layer(x) * tf.math.sqrt(tf.cast(self.dim_model, tf.float32))\n",
        "        x = x + self.position_embeddings[tf.newaxis, :tf.shape(x)[1], :]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_OlPgAXYMJv"
      },
      "source": [
        "#### 4.2 Attention\n",
        "\n",
        "The attention layers all rely on the same foundation which contains a multiple attention head, a normalization layer and an addition layer. The only difference between the different attention layers are the `Q` (query), `K` (key), and `V` (value) inputs which will be sent to the formula:\n",
        "\n",
        "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
        "\n",
        "For this, the `DefaultAttention` class, a class from which all other attention layers will inherit, was created to avoid repeating the same constructor 3 times. You must complete the `call()` functions of each of the subclasses, namely `CrossAttention`, `GlobalSelfAttention` and `DecoderAttention`. To evaluate the values of `K`, `V` and `Q` of each attention layer, refer to the architecture graph.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5SBW-fAbNtUP"
      },
      "outputs": [],
      "source": [
        "class DefaultAttention(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Base attention layer containing attention heads followed by a normalization and addition layer\n",
        "    \"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        self.multiHeadAttention = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "        self.layerNormalization = tf.keras.layers.LayerNormalization()\n",
        "        self.addLayer = tf.keras.layers.Add()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFLZ0q-RNtUP"
      },
      "source": [
        "##### 4.2.1 CrossAttention\n",
        "In the case of the `CrossAttention` layer, the `call` function takes the following inputs as parameters:\n",
        "- `input`: The inputs of the layer, corresponding to the output of the `DecoderAttention` layer\n",
        "- `context`: The output of the encoder\n",
        "- `training`: Boolean value indicating whether the model is training or not.\n",
        "\n",
        "This function should perform the following operations:\n",
        "1. Apply the multiple attention heads layer with the correct values of `K`, `V` and `Q` (Don't forget to pass the `training` argument to the layer).\n",
        "2. Add the output of the attention head layer to the inputs using the `Add` layer\n",
        "3. Normalize everything using the normalization layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Rrq7qz46NtUP"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(DefaultAttention):\n",
        "    \"\"\"\n",
        "    Layer that connects the encoder to the decoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes an attention heads layer followed by a normalization layer\n",
        "        then addition\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, input, context, training):\n",
        "        \"\"\"\n",
        "        Runs the attention layer. Adds attention outputs to the input and\n",
        "         normalizes everything\n",
        "        \"\"\"\n",
        "        # Initialize the query, key and value tensors\n",
        "        Q, K, V = input, context, context\n",
        "\n",
        "        # Call the multi-head attention layer\n",
        "        attention_output = self.multiHeadAttention(query=Q, key=K, value=V, training=training)\n",
        "\n",
        "        # Add the attention output to the input and normalize it\n",
        "        output = self.addLayer([input, attention_output])\n",
        "        output = self.layerNormalization(output)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVaWExw7NtUP"
      },
      "source": [
        "##### 4.2.2 GlobalSelfAttention\n",
        "In the case of the `GlobalSelfAttention` layer, the `call` function takes the following inputs as parameters:\n",
        "- `input`: The inputs of the layer, corresponding to the output of the `DecoderAttention` layer\n",
        "- `training`: Boolean value indicating whether the model is training or not.\n",
        "\n",
        "This function should perform the following operations:\n",
        "1. Apply the multiple attention heads layer with the correct values of `K`, `V` and `Q` (Don't forget to pass the `training` argument to the layer).\n",
        "2. Add the output of the attention head layer to the inputs using the `Add` layer\n",
        "3. Normalize everything using the normalization layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1GBHo7aoNtUP"
      },
      "outputs": [],
      "source": [
        "class GlobalSelfAttention(DefaultAttention):\n",
        "    \"\"\"\n",
        "    Self-attention layer allowing the model to look at other words in\n",
        "    the input phrase when it encodes a specific word\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes a layer of attention heads followed by a layer of\n",
        "        normalization then addition\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, input, training):\n",
        "        \"\"\"\n",
        "        Runs the attention layer. Add attention outputs to input\n",
        "        and normalize everything\n",
        "        \"\"\"\n",
        "        # Initialize the query, key and value tensors\n",
        "        Q = input\n",
        "        K = input\n",
        "        V = input\n",
        "\n",
        "        # Call the multi-head attention layer\n",
        "        attention_output = self.multiHeadAttention(query=Q, value=K, key=V, training=training)\n",
        "\n",
        "        # Add the attention output to the input and normalize it\n",
        "        output = self.addLayer([input, attention_output])\n",
        "        output = self.layerNormalization(output)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcwSdiBkNtUQ"
      },
      "source": [
        "##### 4.2.3 DecoderAttention\n",
        "In the case of the `DecoderAttention` layer, the `call` function takes the following inputs as parameters:\n",
        "- `input`: The inputs of the layer, corresponding to the output of the `DecoderAttention` layer\n",
        "- `training`: Boolean value indicating whether the model is training or not.\n",
        "\n",
        "The implementation of the method is very similar to the `call` function of the `GlobalSelfAttention` class, but differs in one key point: the causal mask. This mask makes it possible in particular not to consider future tokens when the attention mechanism is calculated. This prevents the Transformer from training by knowing the future tokens that it must predict (therefore by “cheating”). This [article](https://medium.com/analytics-vidhya/masking-in-transformers-self-attention-mechanism-bad3c9ec235c) gives more information on the causal mask.\n",
        "\n",
        "This function should perform the following operations:\n",
        "1. Apply the multiple attention heads layer with the correct values of `K`, `V` and `Q` (Don't forget to pass the `training` argument to the layer and enable the causal mask of layer by setting the `use_causal_mask` attribute to `True` when calling the attention layer).\n",
        "2. Add the output of the attention head layer to the inputs using the `Add` layer\n",
        "3. Normalize everything using the normalization layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "o7UdIeC7XBUz"
      },
      "outputs": [],
      "source": [
        "class DecoderAttention(DefaultAttention):\n",
        "    \"\"\"\n",
        "    Attention layer similar to the global self-attention layer, but masking\n",
        "    the data that comes after\n",
        "    \"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes an attention heads layer followed by a normalization layer\n",
        "        then addition\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, input, training):\n",
        "        \"\"\"\n",
        "        Runs the attention layer by hiding the data afterwards. Add the outputs\n",
        "        attention at the entrance and normalizes everything\n",
        "        \"\"\"\n",
        "        # Initialize the query, key and value tensors\n",
        "        Q, K, V = input, input, input\n",
        "\n",
        "        # Call the multi-head attention layer\n",
        "        attention_output = self.multiHeadAttention(query=Q, value=K, key=V, training=training, use_causal_mask=True)\n",
        "\n",
        "        # Add the attention output to the input and normalize it\n",
        "        output = self.addLayer([input, attention_output])\n",
        "        output = self.layerNormalization(output)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCQhCvv5NtUQ"
      },
      "source": [
        "We can test our implementation of attention layers using the following function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "4rXQoKfRNtUQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c513b8b-7cf3-4313-d722-f8e985a235bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross Attention result : \n",
            "tf.Tensor(\n",
            "[[[ 124 -119   -4]]\n",
            "\n",
            " [[ 140  -59  -80]]\n",
            "\n",
            " [[  79   60 -140]]], shape=(3, 1, 3), dtype=int32) \n",
            "\n",
            "Global-Self Attention result : \n",
            "tf.Tensor(\n",
            "[[[ 124 -119   -4]]\n",
            "\n",
            " [[ 140  -59  -80]]\n",
            "\n",
            " [[  79   60 -140]]], shape=(3, 1, 3), dtype=int32) \n",
            "\n",
            "Decoder Attention result : \n",
            "tf.Tensor(\n",
            "[[[ 124 -119   -4]]\n",
            "\n",
            " [[ 140  -59  -80]]\n",
            "\n",
            " [[  79   60 -140]]], shape=(3, 1, 3), dtype=int32) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "def test_attention():\n",
        "    config = {\n",
        "        'num_heads': 3,\n",
        "        'key_dim': 3,\n",
        "        'dropout': 0.1\n",
        "    }\n",
        "    cross_attention = CrossAttention(**config)\n",
        "    global_self_attention = GlobalSelfAttention(**config)\n",
        "    decoder_attention = DecoderAttention(**config)\n",
        "\n",
        "    # Create determinisitc inputs and context\n",
        "    generator = tf.random.Generator.from_seed(1)\n",
        "    input = generator.normal(shape=(3, 1, 3))\n",
        "    context = generator.normal(shape=(3, 1, 3))\n",
        "\n",
        "    # Make attention layer deterministic\n",
        "    layer = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=4, dropout=0.1, kernel_initializer=tf.keras.initializers.ones())\n",
        "    cross_attention.multiHeadAttention = layer\n",
        "    global_self_attention.multiHeadAttention = layer\n",
        "    decoder_attention.multiHeadAttention = layer\n",
        "\n",
        "    outputs_cross_attention = tf.cast(cross_attention(input, context) * 100, tf.int32)\n",
        "    outputs_global_self_attention = tf.cast(global_self_attention(input) * 100, tf.int32)\n",
        "    outputs_decoder_attention = tf.cast(decoder_attention(input) * 100, tf.int32)\n",
        "\n",
        "    print('Cross Attention result : ')\n",
        "    print(outputs_cross_attention, '\\n')\n",
        "\n",
        "    print('Global-Self Attention result : ')\n",
        "    print(outputs_global_self_attention, '\\n')\n",
        "\n",
        "    print('Decoder Attention result : ')\n",
        "    print(outputs_decoder_attention, '\\n')\n",
        "\n",
        "test_attention()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84vLhOVGY7vf"
      },
      "source": [
        "#### 4.3 Feed Forward\n",
        "\n",
        "The Feed Forward layer is, in our case, simply a sequence of 2 dense layers, a dropout layer, an addition layer and a normalization layer. These layers are already initialized in the constructor using a `Sequential` layer which groups several layers and applies them one after the other.\n",
        "\n",
        "The `call` function takes the following inputs as parameters:\n",
        "- `input`: Layer inputs (varies depending on where this layer is located in the architecture)\n",
        "\n",
        "It then returns the result once the transformations are applied on the inputs\n",
        "\n",
        "It performs the following operations:\n",
        "1. Runs the sequential layer initialized in the constructor\n",
        "2. Adds the result of the sequential layer to the inputs\n",
        "3. Normalize everything using the normalization layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "gm_4ethRYKR_"
      },
      "outputs": [],
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Propagation layer at the output of attention layers\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim_model, feed_forward_size, dropout_rate=0.1):\n",
        "        \"\"\"\n",
        "        Initializes layers of dense propagation (with dropout), addition and normalization\n",
        "        Args:\n",
        "            - dim_model: Model dimension (layer output)\n",
        "            - feed_forward_size: Size of the dense propagation layer (input)\n",
        "            - dropout_rate: Ratio of dropout layer entries that\n",
        "            will be initialized to zero randomly\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.seq = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(feed_forward_size, activation='relu'),\n",
        "            tf.keras.layers.Dense(dim_model),\n",
        "            tf.keras.layers.Dropout(dropout_rate)\n",
        "        ])\n",
        "        self.add = tf.keras.layers.Add()\n",
        "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "    def call(self, input):\n",
        "        \"\"\"\n",
        "        Runs propagation layers on the input, adds everything together and normalizes\n",
        "        \"\"\"\n",
        "        # Add the layers and normalize the output\n",
        "        output = self.add([input, self.seq(input)])\n",
        "        output = self.layer_norm(output)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxPcLmhrZq-c"
      },
      "source": [
        "#### 4.4 Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbfMNHvUNtUR"
      },
      "source": [
        "Our Transformer's encoder is actually made up of several layers called `EncoderLayer`. These layers represent a single pass of an encoder. However, the `Encoder` class groups together several of these `EncoderLayer`s to allow the Transformer to capture more complicated contexts between words.\n",
        "\n",
        "We will therefore have to complete the `call` method of the `EncoderLayer` class. This method takes the following parameters as input:\n",
        "- `input`: Layer inputs (notably the output of the `PositionalEmbedding` class)\n",
        "- `training`: Boolean value indicating whether the method is called during training or not\n",
        "\n",
        "It returns inputs once they have passed through all layers (`GlobalSelfAttention`, `FeedForward`)\n",
        "\n",
        "This method should perform the following operations:\n",
        "1. Call the attention layer with inputs\n",
        "2. Call the propagation layer on the output of the attention layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "IIyENqA5Y4za"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Class representing an encoder layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *, dim_model, num_heads, feed_forward_size, dropout_rate=0.1):\n",
        "        \"\"\"\n",
        "        Initializes a self-attention layer followed by a propagation layer\n",
        "\n",
        "        Args:\n",
        "            dim_model: Dimension of model embeddings\n",
        "            num_heads: Number of encoder attention heads\n",
        "            feed_forward_size: Number of feed forward neurons\n",
        "            dropout_rate: Ratio of attention layer entries that will be\n",
        "            randomly initialized to zero\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attention = GlobalSelfAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=dim_model,\n",
        "            dropout=dropout_rate\n",
        "        )\n",
        "\n",
        "        self.ffn = FeedForward(dim_model, feed_forward_size)\n",
        "\n",
        "    def call(self, input, training):\n",
        "        \"\"\"\n",
        "        Runs the attention and propagation layer on the inputs.\n",
        "        The training argument specifies whether the call is made during training\n",
        "        or not (important for the attention layer)\n",
        "        \"\"\"\n",
        "        # Call the global self-attention layer and the feed forward layer\n",
        "        output = self.self_attention(input, training=training)\n",
        "        output = self.ffn(output)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI5qbkU_NtUR"
      },
      "source": [
        "Now, the `Encoder` class takes care of grouping several `EncoderLayer`s to allow the Transformer to infer more complex contexts.\n",
        "\n",
        "The `call` method of the `Encoder` class takes the following parameters as input:\n",
        "- `input`: Layer inputs (corresponding to the token indices of the sentence)\n",
        "- `training`: Boolean value indicating whether the method is called during training or not\n",
        "\n",
        "It returns inputs once they have passed through all encoder layers\n",
        "\n",
        "This method performs the following operations:\n",
        "1. Call position embeddings layer on inputs\n",
        "2. Apply the dropout layer to the result\n",
        "3. Call all layers `EncoderLayer` (the output of one encoder layer becomes the input of another)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "OoIdsp__ZjkE"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Class representing all Transformer encoders\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *, num_layers, dim_model, num_heads, feed_forward_size, vocab_size, dropout_rate=0.1):\n",
        "        \"\"\"\n",
        "        Initializes the position embeddings layer, a dropout layer, and the encoder layers\n",
        "        Args:\n",
        "            num_layers: Number of encoder layers\n",
        "            dim_model: Dimension of model embeddings\n",
        "            num_heads: Number of encoder attention heads\n",
        "            feed_forward_size: Size of the feed forward (output)\n",
        "            vocab_size: Size of the vocabulary (corresponding to the input size of the\n",
        "            position embeddings layer)\n",
        "            dropout_rate: Ratio of dropout layer entries that will be initialized\n",
        "            to zero randomly\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim_model = dim_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.pos_embedding = PositionalEmbedding(input_size=vocab_size, dim_model=dim_model)\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.enc_layers = [EncoderLayer(dim_model=dim_model, num_heads=num_heads, feed_forward_size=feed_forward_size, dropout_rate=dropout_rate) for _ in range(num_layers)]\n",
        "\n",
        "    def call(self, input, training):\n",
        "        \"\"\"\n",
        "        Execute the embeddings and dropouts layer then all the encoder layers\n",
        "        \"\"\"\n",
        "        input = self.dropout(self.pos_embedding(input))\n",
        "        for i in range(self.num_layers):\n",
        "            input = self.enc_layers[i](input, training)\n",
        "\n",
        "        return input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gL0Nvfwcf1x"
      },
      "source": [
        "#### 4.5 Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPk3mPjeNtUS"
      },
      "source": [
        "The decoder of our Transformer is actually made up of several layers called `DecoderLayer`. These layers represent a single pass of a decoder. However, the `Decoder` class groups together several of these `DecoderLayer`s to allow the Transformer to capture more complicated contexts between words.\n",
        "\n",
        "We will therefore have to complete the `call` method of the `DecoderLayer` class. This method takes the following parameters as input:\n",
        "- `input`: Layer inputs\n",
        "- `context`: The context of the attention layers\n",
        "- `training`: Boolean value indicating whether the method is called during training or not\n",
        "\n",
        "It returns inputs once they have passed through all layers (`DecoderAttention`, `CrossAttention`, `FeedForward`)\n",
        "\n",
        "This method should perform the following operations:\n",
        "1. Call the decoder attention layer with inputs\n",
        "2. Call the cross-attention layer\n",
        "3. Call the propagation layer (`FeedForward`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "7t3-gOticXIr"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Class representing a decoder layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *, dim_model, num_heads, feed_forward_size, dropout_rate=0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dim_model: Dimension of model embeddings\n",
        "            num_heads: Number of decoder attention heads\n",
        "            feed_forward_size: Number of feed forward neurons\n",
        "            dropout_rate: Dropout ratio for neurons in the Feed Forward layer\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder_decoder_attention = DecoderAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=dim_model,\n",
        "            dropout=dropout_rate\n",
        "        )\n",
        "\n",
        "        self.cross_attention = CrossAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=dim_model,\n",
        "            dropout=dropout_rate\n",
        "        )\n",
        "\n",
        "        self.ffn = FeedForward(dim_model, feed_forward_size)\n",
        "\n",
        "    def call(self, input, context, training):\n",
        "        \"\"\"\n",
        "        Runs attention layers followed by FFN propagation layers\n",
        "        \"\"\"\n",
        "        # Call the decoder attention layer, the cross attention layer and the feed forward layer\n",
        "        output = self.encoder_decoder_attention(input, training=training)\n",
        "        output = self.cross_attention(output, context, training=training)\n",
        "        output = self.ffn(output)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy20-swPNtUS"
      },
      "source": [
        "The `Decoder` class takes care of grouping several `DecoderLayer`s.\n",
        "\n",
        "The `call` method of the `Decoder` class takes the following parameters as input:\n",
        "- `input`: Layer inputs (corresponding to the token indices of the sentence)\n",
        "- `context`: Context of the attention layers (corresponding to the encoder output)\n",
        "- `training`: Boolean value indicating whether the method is called during training or not\n",
        "\n",
        "It returns inputs once they have passed through all decoder layers\n",
        "\n",
        "This method performs the following operations:\n",
        "1. Call position embeddings layer on inputs\n",
        "2. Apply the dropout layer to the result\n",
        "3. Call the `DecoderLayer` layers successively\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "NdzjzJ2wcwDu"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, *, num_layers, dim_model, num_heads, feed_forward_size, vocab_size, dropout_rate=0.1):\n",
        "        \"\"\"\n",
        "        Initializes the position embeddings layer, a dropout layer, and the encoder layers\n",
        "        Args:\n",
        "            num_layers: Number of decoder layers\n",
        "            dim_model: Dimension of model embeddings\n",
        "            num_heads: Number of encoder attention heads\n",
        "            feed_forward_size: Size of the feed forward (output)\n",
        "            vocab_size: Vocabulary size (corresponding to the input size\n",
        "            of the position embeddings layer)\n",
        "            dropout_rate: Ratio of dropout layer entries that will be\n",
        "            randomly initialized to zero\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim_model = dim_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.pos_embedding = PositionalEmbedding(input_size=vocab_size, dim_model=dim_model)\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dec_layers = [DecoderLayer(dim_model=dim_model, num_heads=num_heads, feed_forward_size=feed_forward_size, dropout_rate=dropout_rate) for x in range(self.num_layers)]\n",
        "\n",
        "        self.last_attn_scores = None\n",
        "\n",
        "    def call(self, input, context, training):\n",
        "        \"\"\"\n",
        "        Executes the dips and dropouts layer\n",
        "        then all layers of decoders\n",
        "        \"\"\"\n",
        "        input = self.dropout(self.pos_embedding(input))\n",
        "        for i in range(self.num_layers):\n",
        "            input = self.dec_layers[i](input, context, training)\n",
        "\n",
        "        return input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVZN8skQdkvp"
      },
      "source": [
        "#### 4.6 Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbuM3NclNtUT"
      },
      "source": [
        "The Transformer is now ready to be created. The constructor already takes care of initializing all the attributes necessary for its operation.\n",
        "\n",
        "The `call` function takes the following arguments as inputs:\n",
        "- `inputs`: The model inputs in the form of a tuple grouping the sparql input and the English input (`inputs = (sparql, english)`)\n",
        "- `training`: Boolean value indicating whether the model is training or not\n",
        "\n",
        "The `call` method must:\n",
        "1. Separate inputs received into sparql and english\n",
        "2. Send sparql sentences to the encoder\n",
        "3. Send the English sentences to the decoder with the encoder output as context\n",
        "4. Send the decoder output to the dense layer initialized in the constructor (`self.dense_layer`)\n",
        "5. Call the `drop_mask` function with the probabilities generated by the dense layer as argument (by removing the `_keras_mask` attribute from the probabilities generated by the dense layer, we prevent the model from using this mask when calculating the metrics and cost)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "NMNM6hDldKs6"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Class representing the Transformer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *, num_layers, dim_model, num_heads, feed_forward_size,\n",
        "                input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "        \"\"\"\n",
        "        Initializes the encoder and decoder layers and the final dense layer\n",
        "        Args:\n",
        "            num_layers: Number of decoder layers\n",
        "            dim_model: Dimension of model embeddings\n",
        "            num_heads: Number of encoder and decoder attention heads\n",
        "            feed_forward_size: Size of the feed forward (output)\n",
        "            input_vocab_size: Size of the input vocabulary\n",
        "            target_vocab_Size: Size of the output vocabulary\n",
        "            dropout_rate: Ratio of dropout layer entries that will be\n",
        "            randomly initialized to zero\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(num_layers=num_layers, dim_model=dim_model,\n",
        "                            num_heads=num_heads, feed_forward_size=feed_forward_size,\n",
        "                            vocab_size=input_vocab_size,\n",
        "                            dropout_rate=dropout_rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers=num_layers, dim_model=dim_model,\n",
        "                            num_heads=num_heads, feed_forward_size=feed_forward_size,\n",
        "                            vocab_size=target_vocab_size,\n",
        "                            dropout_rate=dropout_rate)\n",
        "\n",
        "        self.dense_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        \"\"\"\n",
        "        Call the encoder and decoder layers with the correct inputs and\n",
        "        context as well as the final dense layer\n",
        "        \"\"\"\n",
        "        # Call the encoder and decoder layers\n",
        "        sparql = inputs[0]\n",
        "        english = inputs[1]\n",
        "        sparql = self.encoder(sparql, training=training)\n",
        "        english = self.decoder(english, sparql, training=training)\n",
        "\n",
        "        # Call the dense layer\n",
        "        english = self.dense_layer(english)\n",
        "\n",
        "        self.drop_mask(training, english)\n",
        "\n",
        "        return english\n",
        "\n",
        "    def drop_mask(self, training, probabilities):\n",
        "        if not training:\n",
        "            try:\n",
        "                del probabilities._keras_mask\n",
        "            except AttributeError:\n",
        "                pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlt3XooMNtUT"
      },
      "source": [
        "We can test our final Transformer implementation with the following function. **Be careful, just because you get the right results doesn't mean there are no bugs in your implementation, but it's already a good sign**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "WjJHTnSzNtUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d83cf861-4909-4da3-9504-287ee42d8638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input\n",
            "output\n",
            "tf.Tensor(\n",
            "[[ 0.43842277 -0.53439844]\n",
            " [-0.07710262  1.5658045 ]], shape=(2, 2), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-0.79253083]\n",
            " [ 0.37646857]], shape=(2, 1), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[ 0.00026987 -0.03291528]]\n",
            "\n",
            " [[ 0.00026987 -0.03291528]]], shape=(2, 1, 2), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "def test_transformer():\n",
        "\n",
        "    config = {\n",
        "        'num_layers': 2,\n",
        "        'dim_model': 2,\n",
        "        'num_heads': 2,\n",
        "        'feed_forward_size': 2,\n",
        "        'input_vocab_size': 2,\n",
        "        'target_vocab_size': 2,\n",
        "        'dropout_rate': 0.1\n",
        "    }\n",
        "\n",
        "    initializer = tf.keras.initializers.glorot_normal(42)\n",
        "\n",
        "    feed_forward = FeedForward(2, 2, 0.1)\n",
        "    feed_forward.seq = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(2, activation='relu', kernel_initializer=initializer, use_bias=False),\n",
        "        tf.keras.layers.Dense(2, kernel_initializer=initializer, use_bias=False),\n",
        "        tf.keras.layers.Dropout(0.1, seed=42)\n",
        "    ])\n",
        "    feed_forward.add = tf.keras.layers.Add()\n",
        "    feed_forward.layer_norm = tf.keras.layers.LayerNormalization(beta_initializer=initializer, gamma_initializer=initializer)\n",
        "\n",
        "    transformer = Transformer(**config)\n",
        "\n",
        "    transformer.encoder.pos_embedding.embedding_layer = tf.keras.layers.Embedding(2, 2, embeddings_initializer=initializer, mask_zero=False)\n",
        "    for l in transformer.encoder.enc_layers:\n",
        "        l.self_attention = GlobalSelfAttention(num_heads=2, key_dim=2, dropout=0.1, kernel_initializer=initializer)\n",
        "        l.ffn = feed_forward\n",
        "    transformer.encoder.dropout = tf.keras.layers.Dropout(0.1, seed=42)\n",
        "\n",
        "    transformer.decoder.pos_embedding.embedding_layer = tf.keras.layers.Embedding(2, 2, embeddings_initializer=initializer, mask_zero=True)\n",
        "    for l in transformer.decoder.dec_layers:\n",
        "        l.cross_attention = CrossAttention(num_heads=2, key_dim=2, dropout=0.1, kernel_initializer=initializer)\n",
        "        l.encoder_decoder_attention = DecoderAttention(num_heads=2, key_dim=2, dropout=0.1, kernel_initializer=initializer)\n",
        "        l.ffn = feed_forward\n",
        "\n",
        "    transformer.dense_layer = tf.keras.layers.Dense(2, kernel_initializer=initializer, use_bias=False)\n",
        "    transformer.decoder.dropout = tf.keras.layers.Dropout(0.1, seed=42)\n",
        "\n",
        "    # Create determinisitc inputs and context\n",
        "    generator = tf.random.Generator.from_seed(1)\n",
        "    input = generator.normal(shape=(2, 2))\n",
        "    context = generator.normal(shape=(2, 1))\n",
        "\n",
        "    print(input)\n",
        "    print(context)\n",
        "\n",
        "    input_transformer = (input, context)\n",
        "    output = transformer(input_transformer, training=False)\n",
        "    print(output)\n",
        "\n",
        "test_transformer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6mVRb5xNtUT"
      },
      "source": [
        "```\n",
        "tf.Tensor(\n",
        "[[[[ 0.00026983 -0.03291529]\n",
        "   [ 0.00026987 -0.03291498]]\n",
        "\n",
        "  [[ 0.00026987 -0.03291498]\n",
        "   [ 0.00026987 -0.03291498]]]\n",
        "\n",
        "\n",
        " [[[ 0.00026983 -0.03291529]\n",
        "   [ 0.00026987 -0.03291498]]\n",
        "\n",
        "  [[ 0.00026987 -0.03291498]\n",
        "   [ 0.00026987 -0.03291498]]]], shape=(2, 2, 2, 2), dtype=float32)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCaWH9jCd51q"
      },
      "source": [
        "#### 4.7 Scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlaJSmAuNtUU"
      },
      "source": [
        "The `Scheduler` class allows, among other things, to update the learning rate of the model during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "R7KODnRNdeHW"
      },
      "outputs": [],
      "source": [
        "class Scheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, dim_model, warmup_steps):\n",
        "        super().__init__()\n",
        "        self.dim_model = tf.cast(dim_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        return tf.math.rsqrt(self.dim_model) * tf.math.minimum(tf.math.rsqrt(step), step * (self.warmup_steps ** -1.5))\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'd_model': self.dim_model,\n",
        "            'warmup_steps': self.warmup_steps,\n",
        "        }\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-wLAeNReP_s"
      },
      "source": [
        "### 5. Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO8vvlCFNtUU"
      },
      "source": [
        "Now it's time to create the translator that will transform sparql queries into English. To do this, you will need to complete 4 methods of the `Translator` class, namely the `prepare`, `fit` and `translate` methods.\n",
        "___\n",
        "\n",
        "The `prepare` function receives the training and validation data in the form of a pandas DataFrame and takes care of:\n",
        "1. Call the preprocessor on the training and validation data\n",
        "2. Create a `tf.Dataset` object containing a tuple of sparql queries and questions in English for the training and validation set\n",
        "3. Send the 2 created datasets (training and validation) to the batcher\n",
        "\n",
        "It returns a tuple containing the training and validation batches\n",
        "\n",
        "___\n",
        "\n",
        "The `fit` function simply trains the model with the training and validation data passed as parameters.\n",
        "___\n",
        "\n",
        "The `translate` function takes care of translating a series of sparql data into English. To do this, several steps must be carried out. She must :\n",
        "1. Apply the preprocessor on the given test set\n",
        "2. Create batches using the test batcher\n",
        "3. For each value in the created batches\n",
        "   - Extract the contents of the tuple. Remember that what is output by the `prepare_batch` method in the case of a test batcher is a tuple of the form (SPARQL sentence, English sentence) where initially, the English sentence is initialized with the starting token\n",
        "   - Send contexts and sentences to the Transformer so that it predicts the next token\n",
        "   - Concatenate together all the tokens predicted by the Transformer to generate the translation\n",
        "4. Reduce the size of the predictions to remove everything that comes after the end token generated by the Transformer (if no end token is generated, the translation does not need to be trimmed)\n",
        "5. Transform predicted tokens into words using the right tokenizer\n",
        "6. Undo initial transformations done using pre-processing\n",
        "\n",
        "\n",
        "The `masked_loss` and `masked_accuracy` functions are provided to you and allow you to evaluate the accuracy of the Transformer by evaluating a loss function specific to the Transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "i_Ta5Z0YfnA1"
      },
      "outputs": [],
      "source": [
        "class Translator:\n",
        "\n",
        "    num_layers = 4\n",
        "    dim_model = 128\n",
        "    feed_forward_size = 512\n",
        "    num_heads = 6\n",
        "    dropout_rate = 0.1\n",
        "    input_vocab_size = 8000\n",
        "    target_vocab_size = 8000\n",
        "    batch_size = 64\n",
        "    batch_size_test = 64\n",
        "    buffer_size = 20000\n",
        "    buffer_size_test = None\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the preprocessor, tokenizers, batchers and Transformer\n",
        "        with the right settings\n",
        "        \"\"\"\n",
        "\n",
        "        self.pre_processor = Preprocessor()\n",
        "\n",
        "        self.tokenizers = GroupedTokenizers(\n",
        "            LanguageTokenizer.reserved_tokens,\n",
        "            root + 'language_vocab_english.txt',\n",
        "            root + 'language_vocab_sparql.txt'\n",
        "        )\n",
        "\n",
        "        self.train_batcher = Batcher(tokenizers=self.tokenizers, train=True, max_tokens=Translator.dim_model, batch_size=Translator.batch_size, buffer_size=Translator.buffer_size)\n",
        "        self.test_batcher = Batcher(tokenizers=self.tokenizers, train=False, max_tokens=Translator.dim_model, batch_size=Translator.batch_size_test, buffer_size=Translator.buffer_size_test)\n",
        "\n",
        "        self.transformer = Transformer(\n",
        "            num_layers=Translator.num_layers,\n",
        "            dim_model=Translator.dim_model,\n",
        "            num_heads=Translator.num_heads,\n",
        "            feed_forward_size=Translator.feed_forward_size,\n",
        "            input_vocab_size=Translator.input_vocab_size,\n",
        "            target_vocab_size=Translator.target_vocab_size,\n",
        "            dropout_rate=Translator.dropout_rate)\n",
        "\n",
        "        self.scheduler = Scheduler(Translator.dim_model, 4000)\n",
        "        self.optimizer = tf.keras.optimizers.Adam(self.scheduler, beta_1=0.9, beta_2=0.95, epsilon=1e-9)\n",
        "\n",
        "        self.transformer.compile(\n",
        "            loss=Translator.masked_loss,\n",
        "            optimizer=self.optimizer,\n",
        "            metrics=[Translator.masked_accuracy])\n",
        "\n",
        "        self.end = self.tokenizers.sparql.tokenize([''])[0][1][tf.newaxis]\n",
        "\n",
        "    def prepare(self, train: pd.DataFrame, val: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Prepares validation and training sets for training\n",
        "        by sending them to the preprocessor and batcher\n",
        "        Args:\n",
        "            - train: Training DataFrame with sparql (input) and English (output) columns\n",
        "            - val: Validation DataFrame with sparql (input) and English (output) columns\n",
        "\n",
        "        Returns:\n",
        "            Tuple containing the training batches and the validation batches\n",
        "        \"\"\"\n",
        "        # Preprocess the data\n",
        "        train = self.pre_processor.transform_dataframe(train)\n",
        "        val = self.pre_processor.transform_dataframe(val)\n",
        "\n",
        "        # Change from dataframe to dataset\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices((train['sparql'], train['english']))\n",
        "        val_dataset = tf.data.Dataset.from_tensor_slices((val['sparql'], val['english']))\n",
        "\n",
        "        # make the batches\n",
        "        train_batches = self.train_batcher.make_batches(train_dataset)\n",
        "        val_batches = self.test_batcher.make_batches(val_dataset)\n",
        "\n",
        "        return train_batches, val_batches\n",
        "\n",
        "    def fit(self, training, validation, epochs=50):\n",
        "        \"\"\"\n",
        "        Train the model using the training set and validate the result\n",
        "        \"\"\"\n",
        "        # Train the model\n",
        "        self.transformer.fit(x=training,\n",
        "                             batch_size=Translator.batch_size,\n",
        "                             epochs=epochs,\n",
        "                             #validation_data=validation,\n",
        "                             #validation_batch_size=Translator.batch_size_test\n",
        "                             )\n",
        "\n",
        "    def translate(self, sparql: pd.Series):\n",
        "        \"\"\"\n",
        "        Translates a series of sparql queries into English\n",
        "        \"\"\"\n",
        "        # Pre-process the sparql sentenses\n",
        "        sparql_processed = sparql.apply(self.pre_processor.transform_sparql)\n",
        "\n",
        "        # Make a dataset\n",
        "        sparql_dataset = tf.data.Dataset.from_tensor_slices(sparql_processed)\n",
        "\n",
        "        # Create batches\n",
        "        batches = self.test_batcher.make_batches(sparql_dataset)\n",
        "\n",
        "        # For each batch\n",
        "        output_list = []\n",
        "        for batch in batches:\n",
        "            for item1, item2 in zip(batch[0], batch[1]):\n",
        "\n",
        "                item1 = tf.expand_dims(item1, axis=0)\n",
        "                item2 = tf.expand_dims(item2, axis=0)\n",
        "                # tf.print(item1)\n",
        "                # tf.print(item2)\n",
        "\n",
        "                output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "                output_array = output_array.write(0, LanguageTokenizer.START)\n",
        "\n",
        "                # Predict one word at a time until we match the end-of-sentence token\n",
        "                for i in tf.range(Translator.dim_model):\n",
        "                  output = tf.transpose(output_array.stack())\n",
        "                  output = tf.expand_dims(output, axis=0)\n",
        "                  predictions = self.transformer([item1, output], training=False)\n",
        "\n",
        "                  # Select the last token from the seq_len dimension.\n",
        "                  predictions = predictions[:, -1:, :]  # Shape (batch_size, 1, vocab_size).\n",
        "                  predicted_id = tf.argmax(predictions, axis=-1)[0][0]\n",
        "\n",
        "                  # Concatenate the predicted_id to the output which is given to the\n",
        "                  # decoder as its input.\n",
        "                  output_array = output_array.write(i+1, predicted_id)\n",
        "                  if predicted_id == LanguageTokenizer.END:\n",
        "                    break\n",
        "\n",
        "                output = tf.transpose(output_array.stack())\n",
        "                output_list.append(output)\n",
        "                output_array = output_array.mark_used()\n",
        "\n",
        "                text = self.tokenizers.english.detokenize(tf.expand_dims(output, axis=0))\n",
        "                input_text = self.tokenizers.sparql.detokenize(tf.expand_dims(item1, axis=0))\n",
        "        return output_list\n",
        "\n",
        "    def masked_loss(label, pred):\n",
        "        mask = label != 0\n",
        "        loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "            from_logits=True, reduction='none')\n",
        "        loss = loss_object(label, pred)\n",
        "\n",
        "        mask = tf.cast(mask, dtype=loss.dtype)\n",
        "        loss *= mask\n",
        "\n",
        "        loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "        return loss\n",
        "\n",
        "    def masked_accuracy(label, pred):\n",
        "        pred = tf.argmax(pred, axis=2)\n",
        "        label = tf.cast(label, pred.dtype)\n",
        "        match = label == pred\n",
        "\n",
        "        mask = label != 0\n",
        "\n",
        "        match = match & mask\n",
        "\n",
        "        match = tf.cast(match, dtype=tf.float32)\n",
        "        mask = tf.cast(mask, dtype=tf.float32)\n",
        "        return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgcU3gQ8NtUU"
      },
      "source": [
        "#### 5.1 Data preparation\n",
        "\n",
        "Then execute the cell below to now create an instance of the `Translator` class, load the training and validation data and prepare the data for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "bPVlAR77eRlt"
      },
      "outputs": [],
      "source": [
        "translator = Translator()\n",
        "\n",
        "data_loader = DataLoader(\n",
        "    training_path=root + 'train.csv',\n",
        "    validation_path=root + 'validation.csv'\n",
        ")\n",
        "\n",
        "train_batch, val_batch = translator.prepare(data_loader.train, data_loader.val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7aXsKYJNtUV"
      },
      "source": [
        "#### 5.2 Training\n",
        "\n",
        "Train the model with the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "1u45C28kr0vS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbf457ae-71f8-4c42-d523-7fe3fdeaf81e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "63/63 [==============================] - 45s 231ms/step - loss: 8.8492 - masked_accuracy: 0.0743\n",
            "Epoch 2/50\n",
            "63/63 [==============================] - 13s 206ms/step - loss: 8.3661 - masked_accuracy: 0.1829\n",
            "Epoch 3/50\n",
            "63/63 [==============================] - 12s 186ms/step - loss: 7.7611 - masked_accuracy: 0.2496\n",
            "Epoch 4/50\n",
            "63/63 [==============================] - 11s 166ms/step - loss: 6.9278 - masked_accuracy: 0.2904\n",
            "Epoch 5/50\n",
            "63/63 [==============================] - 11s 169ms/step - loss: 6.0144 - masked_accuracy: 0.3121\n",
            "Epoch 6/50\n",
            "63/63 [==============================] - 12s 195ms/step - loss: 5.1637 - masked_accuracy: 0.3349\n",
            "Epoch 7/50\n",
            "63/63 [==============================] - 11s 176ms/step - loss: 4.4393 - masked_accuracy: 0.3760\n",
            "Epoch 8/50\n",
            "63/63 [==============================] - 12s 188ms/step - loss: 3.8007 - masked_accuracy: 0.4282\n",
            "Epoch 9/50\n",
            "63/63 [==============================] - 12s 193ms/step - loss: 3.2881 - masked_accuracy: 0.4614\n",
            "Epoch 10/50\n",
            "63/63 [==============================] - 11s 169ms/step - loss: 2.9732 - masked_accuracy: 0.4770\n",
            "Epoch 11/50\n",
            "63/63 [==============================] - 11s 171ms/step - loss: 2.7920 - masked_accuracy: 0.4888\n",
            "Epoch 12/50\n",
            "63/63 [==============================] - 11s 176ms/step - loss: 2.6741 - masked_accuracy: 0.5014\n",
            "Epoch 13/50\n",
            "63/63 [==============================] - 11s 167ms/step - loss: 2.5756 - masked_accuracy: 0.5120\n",
            "Epoch 14/50\n",
            "63/63 [==============================] - 10s 163ms/step - loss: 2.4964 - masked_accuracy: 0.5228\n",
            "Epoch 15/50\n",
            "63/63 [==============================] - 11s 166ms/step - loss: 2.4071 - masked_accuracy: 0.5361\n",
            "Epoch 16/50\n",
            "63/63 [==============================] - 11s 172ms/step - loss: 2.3204 - masked_accuracy: 0.5500\n",
            "Epoch 17/50\n",
            "63/63 [==============================] - 11s 168ms/step - loss: 2.2179 - masked_accuracy: 0.5673\n",
            "Epoch 18/50\n",
            "63/63 [==============================] - 13s 202ms/step - loss: 2.0927 - masked_accuracy: 0.5900\n",
            "Epoch 19/50\n",
            "63/63 [==============================] - 10s 163ms/step - loss: 1.9434 - masked_accuracy: 0.6163\n",
            "Epoch 20/50\n",
            "63/63 [==============================] - 11s 172ms/step - loss: 1.7781 - masked_accuracy: 0.6403\n",
            "Epoch 21/50\n",
            "63/63 [==============================] - 12s 189ms/step - loss: 1.6143 - masked_accuracy: 0.6658\n",
            "Epoch 22/50\n",
            "63/63 [==============================] - 11s 176ms/step - loss: 1.4288 - masked_accuracy: 0.6993\n",
            "Epoch 23/50\n",
            "63/63 [==============================] - 11s 171ms/step - loss: 1.2611 - masked_accuracy: 0.7298\n",
            "Epoch 24/50\n",
            "63/63 [==============================] - 10s 163ms/step - loss: 1.1035 - masked_accuracy: 0.7627\n",
            "Epoch 25/50\n",
            "63/63 [==============================] - 10s 164ms/step - loss: 0.9438 - masked_accuracy: 0.7959\n",
            "Epoch 26/50\n",
            "63/63 [==============================] - 10s 165ms/step - loss: 0.8224 - masked_accuracy: 0.8207\n",
            "Epoch 27/50\n",
            "63/63 [==============================] - 11s 175ms/step - loss: 0.7037 - masked_accuracy: 0.8425\n",
            "Epoch 28/50\n",
            "63/63 [==============================] - 11s 179ms/step - loss: 0.6256 - masked_accuracy: 0.8568\n",
            "Epoch 29/50\n",
            "63/63 [==============================] - 11s 175ms/step - loss: 0.5436 - masked_accuracy: 0.8719\n",
            "Epoch 30/50\n",
            "63/63 [==============================] - 11s 166ms/step - loss: 0.4923 - masked_accuracy: 0.8803\n",
            "Epoch 31/50\n",
            "63/63 [==============================] - 11s 168ms/step - loss: 0.4455 - masked_accuracy: 0.8889\n",
            "Epoch 32/50\n",
            "63/63 [==============================] - 10s 164ms/step - loss: 0.4063 - masked_accuracy: 0.8965\n",
            "Epoch 33/50\n",
            "63/63 [==============================] - 11s 174ms/step - loss: 0.3940 - masked_accuracy: 0.8969\n",
            "Epoch 34/50\n",
            "63/63 [==============================] - 11s 168ms/step - loss: 0.3367 - masked_accuracy: 0.9110\n",
            "Epoch 35/50\n",
            "63/63 [==============================] - 10s 166ms/step - loss: 0.3283 - masked_accuracy: 0.9133\n",
            "Epoch 36/50\n",
            "63/63 [==============================] - 10s 166ms/step - loss: 0.3222 - masked_accuracy: 0.9132\n",
            "Epoch 37/50\n",
            "63/63 [==============================] - 10s 163ms/step - loss: 0.2797 - masked_accuracy: 0.9232\n",
            "Epoch 38/50\n",
            "63/63 [==============================] - 10s 166ms/step - loss: 0.2725 - masked_accuracy: 0.9252\n",
            "Epoch 39/50\n",
            "63/63 [==============================] - 10s 163ms/step - loss: 0.2701 - masked_accuracy: 0.9243\n",
            "Epoch 40/50\n",
            "63/63 [==============================] - 11s 167ms/step - loss: 0.2423 - masked_accuracy: 0.9323\n",
            "Epoch 41/50\n",
            "63/63 [==============================] - 12s 182ms/step - loss: 0.2379 - masked_accuracy: 0.9332\n",
            "Epoch 42/50\n",
            "63/63 [==============================] - 11s 170ms/step - loss: 0.2348 - masked_accuracy: 0.9335\n",
            "Epoch 43/50\n",
            "63/63 [==============================] - 10s 165ms/step - loss: 0.2183 - masked_accuracy: 0.9379\n",
            "Epoch 44/50\n",
            "63/63 [==============================] - 11s 168ms/step - loss: 0.2088 - masked_accuracy: 0.9402\n",
            "Epoch 45/50\n",
            "63/63 [==============================] - 10s 163ms/step - loss: 0.2001 - masked_accuracy: 0.9423\n",
            "Epoch 46/50\n",
            "63/63 [==============================] - 10s 164ms/step - loss: 0.1913 - masked_accuracy: 0.9445\n",
            "Epoch 47/50\n",
            "63/63 [==============================] - 10s 164ms/step - loss: 0.1854 - masked_accuracy: 0.9466\n",
            "Epoch 48/50\n",
            "63/63 [==============================] - 10s 165ms/step - loss: 0.1803 - masked_accuracy: 0.9481\n",
            "Epoch 49/50\n",
            "63/63 [==============================] - 10s 165ms/step - loss: 0.1674 - masked_accuracy: 0.9515\n",
            "Epoch 50/50\n",
            "63/63 [==============================] - 11s 170ms/step - loss: 0.1712 - masked_accuracy: 0.9502\n"
          ]
        }
      ],
      "source": [
        "translator.fit(train_batch, val_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLnTdbjyNtUV"
      },
      "source": [
        "#### 5.3 Translation\n",
        "\n",
        "Perform test data translation to validate model effectiveness"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = translator.translate(data_loader.val['sparql'])"
      ],
      "metadata": {
        "id": "Edrb0SU5YWnz"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = pd.Series(predictions)\n",
        "a_df = pd.DataFrame(a)\n",
        "a_df"
      ],
      "metadata": {
        "id": "rpbOhWArODxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_predictions = pd.concat([pd.DataFrame(predictions), data_loader.val], axis=1)\n",
        "formatted_predictions.drop(['id', 'sparql'], inplace=True, axis=1)\n",
        "formatted_predictions.rename(columns={0:'prediction', 'english':'target_text'}, inplace=True)\n",
        "formatted_predictions.head(10)"
      ],
      "metadata": {
        "id": "rVGjFuG1UNxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZSxn1aTholW"
      },
      "source": [
        "### 6. Evaluation: BLUE Metric\n",
        "\n",
        "To evaluate the effectiveness of translations, the BLEU metric will be used. The formula is given below:\n",
        "$$BLUE = BP * exp \\Big( \\sum_{n=1}^{N} w_n log p_n \\Big)$$\n",
        "\n",
        "where $p_n$ is the modified precision for the n-gram (corresponding to the ratio of the maximum frequency of the n-gram in each reference sentence to the frequency of the n-gram). Let $r$ then be the number of words in the target sentence and $c$ as the number of words in the predicted sentence. If $c>r$, then BP is 1. Otherwise $BP = exp(1 - \\frac{r}{c})$.\n",
        "\n",
        "The values of the weights $w_n$ are what give the different variations of the BLUE metric. In our case, the BLEU-3 metric will be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoCj2WfhiMKW"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(data: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Evaluates model accuracy using the BLUE metric\n",
        "    Args:\n",
        "        - data: DataFrame containing two columns (predictions and target_text)\n",
        "\n",
        "    Returns:\n",
        "        The average BLUE score\n",
        "    \"\"\"\n",
        "    weights = (1/3, 1/3, 1/3) # Use Bleu-3\n",
        "    scores = np.zeros(data.shape[0])\n",
        "    index = 0\n",
        "    for iter, row in data.iterrows():\n",
        "        if not pd.notnull(row['prediction']):\n",
        "            continue\n",
        "        prediction = row['prediction'].split()\n",
        "        target_text = row['target_text'].split()\n",
        "\n",
        "        scores[index] = sentence_bleu([target_text], prediction, weights=weights)\n",
        "\n",
        "        index += 1\n",
        "    return np.mean(scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62ICkj3qNtUW"
      },
      "source": [
        "#### 6.1 Model evaluation\n",
        "\n",
        "Call the `evaluate_model` function on your model's predictions to evaluate its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZVLOVd2wSfd"
      },
      "outputs": [],
      "source": [
        "evaluate_model(formatted_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN8Bcg6VNtUW"
      },
      "source": [
        "#### 6.2. Error analysis\n",
        "Analyze model translations and errors. Implement a statistical analysis (in the form of your choice) which displays categories of errors and their % occurrence among all possible errors. You can direct your function to describe specific dimensions. For example: are the errors more often on the elements of the \"dbx\" knowledge base or on the rest of the tokens? Are the errors due to elements that are not seen in training?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS7B_H9ZNtUW"
      },
      "source": [
        "> TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN2sFxJhNtUW"
      },
      "source": [
        "#### 6.3 Improvement\n",
        "Give possible solutions to improve the BLEU score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJqNDCTFNtUW"
      },
      "source": [
        "A larger training set could allow the model to better learn the trigrams from the training set, which would increase the accuracy percentage with the expected trigrams. Since the degree of similarity of the trigrams of the prediction set to the expected set is a factor in the BLEU-3 metric, its score will be higher. It would also be beneficial to adjust the BLEU metric used depending on the size of the training set. For example, if the training set is small, it would be more beneficial to use a BLUE-2 or BLUE-3 metric since smaller n-grams will be much more common."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a88qkoakNtUW"
      },
      "source": [
        "## END\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}